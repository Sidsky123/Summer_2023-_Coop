{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4a7b878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: (2110, 14)\n",
      "age        -0.493310\n",
      "sex        -1.855291\n",
      "cp         -0.257508\n",
      "trestbps    1.493132\n",
      "chol       -0.241968\n",
      "fbs        -0.490700\n",
      "restecg    -0.779254\n",
      "thalch      0.668772\n",
      "exang      -0.796772\n",
      "oldpeak     0.087912\n",
      "slope       0.528227\n",
      "ca         -0.276244\n",
      "thal       -0.528078\n",
      "num         1.000000\n",
      "Name: 1, dtype: float64\n",
      "(1489, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg. Loss: 0.6989\n",
      "epsilon list is  [6.049880161691039]\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Avg. Loss: 0.6900\n",
      "epsilon list is  [6.049880161691039, 6.784254555890441]\n",
      "Test Accuracy: 0.4866\n",
      "Confusion Matrix:\n",
      "[[141   1]\n",
      " [152   4]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.99      0.65       142\n",
      "         1.0       0.80      0.03      0.05       156\n",
      "\n",
      "    accuracy                           0.49       298\n",
      "   macro avg       0.64      0.51      0.35       298\n",
      "weighted avg       0.65      0.49      0.33       298\n",
      "\n",
      "Test Accuracy: 0.4866\n",
      "Confusion Matrix:\n",
      "[[141   1]\n",
      " [152   4]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.99      0.65       142\n",
      "         1.0       0.80      0.03      0.05       156\n",
      "\n",
      "    accuracy                           0.49       298\n",
      "   macro avg       0.64      0.51      0.35       298\n",
      "weighted avg       0.65      0.49      0.33       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.4865771812080537]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg. Loss: 0.6780\n",
      "epsilon list is  [6.049880161691039, 6.784254555890441, 7.350203436957572]\n",
      "Test Accuracy: 0.7148\n",
      "Confusion Matrix:\n",
      "[[139   3]\n",
      " [ 82  74]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.98      0.77       142\n",
      "         1.0       0.96      0.47      0.64       156\n",
      "\n",
      "    accuracy                           0.71       298\n",
      "   macro avg       0.79      0.73      0.70       298\n",
      "weighted avg       0.80      0.71      0.70       298\n",
      "\n",
      "Test Accuracy: 0.7148\n",
      "Confusion Matrix:\n",
      "[[139   3]\n",
      " [ 82  74]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.98      0.77       142\n",
      "         1.0       0.96      0.47      0.64       156\n",
      "\n",
      "    accuracy                           0.71       298\n",
      "   macro avg       0.79      0.73      0.70       298\n",
      "weighted avg       0.80      0.71      0.70       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.4865771812080537, 0.714765100671141]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Avg. Loss: 0.6473\n",
      "epsilon list is  [6.049880161691039, 6.784254555890441, 7.350203436957572, 7.830958768720515]\n",
      "Test Accuracy: 0.8221\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 30 126]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.84      0.82       142\n",
      "         1.0       0.85      0.81      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8221\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 30 126]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.84      0.82       142\n",
      "         1.0       0.85      0.81      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.4865771812080537, 0.714765100671141, 0.8221476510067114]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Avg. Loss: 0.5817\n",
      "epsilon list is  [6.049880161691039, 6.784254555890441, 7.350203436957572, 7.830958768720515, 8.25879465414123]\n",
      "Test Accuracy: 0.7953\n",
      "Confusion Matrix:\n",
      "[[103  39]\n",
      " [ 22 134]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.73      0.77       142\n",
      "         1.0       0.77      0.86      0.81       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.80      0.79      0.79       298\n",
      "weighted avg       0.80      0.80      0.79       298\n",
      "\n",
      "Test Accuracy: 0.7953\n",
      "Confusion Matrix:\n",
      "[[103  39]\n",
      " [ 22 134]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.73      0.77       142\n",
      "         1.0       0.77      0.86      0.81       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.80      0.79      0.79       298\n",
      "weighted avg       0.80      0.80      0.79       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.4865771812080537, 0.714765100671141, 0.8221476510067114, 0.7953020134228188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Avg. Loss: 0.4837\n",
      "epsilon list is  [6.049880161691039, 6.784254555890441, 7.350203436957572, 7.830958768720515, 8.25879465414123, 8.649892009270342]\n",
      "Test Accuracy: 0.8121\n",
      "Confusion Matrix:\n",
      "[[108  34]\n",
      " [ 22 134]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.76      0.79       142\n",
      "         1.0       0.80      0.86      0.83       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Test Accuracy: 0.8121\n",
      "Confusion Matrix:\n",
      "[[108  34]\n",
      " [ 22 134]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.76      0.79       142\n",
      "         1.0       0.80      0.86      0.83       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.4865771812080537, 0.714765100671141, 0.8221476510067114, 0.7953020134228188, 0.8120805369127517]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Avg. Loss: 0.4390\n",
      "epsilon list is  [6.049880161691039, 6.784254555890441, 7.350203436957572, 7.830958768720515, 8.25879465414123, 8.649892009270342, 9.01362725014036]\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.4865771812080537, 0.714765100671141, 0.8221476510067114, 0.7953020134228188, 0.8120805369127517, 0.8355704697986577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Avg. Loss: 0.4355\n",
      "epsilon list is  [6.049880161691039, 6.784254555890441, 7.350203436957572, 7.830958768720515, 8.25879465414123, 8.649892009270342, 9.01362725014036, 9.355934235483865]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.4865771812080537, 0.714765100671141, 0.8221476510067114, 0.7953020134228188, 0.8120805369127517, 0.8355704697986577, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Avg. Loss: 0.4877\n",
      "epsilon list is  [6.049880161691039, 6.784254555890441, 7.350203436957572, 7.830958768720515, 8.25879465414123, 8.649892009270342, 9.01362725014036, 9.355934235483865, 9.680900995096724]\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.4865771812080537, 0.714765100671141, 0.8221476510067114, 0.7953020134228188, 0.8120805369127517, 0.8355704697986577, 0.8322147651006712, 0.8355704697986577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg. Loss: 0.6908\n",
      "epsilon list is  [4.673786299411615]\n",
      "Test Accuracy: 0.6711\n",
      "Confusion Matrix:\n",
      "[[ 90  52]\n",
      " [ 46 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.63      0.65       142\n",
      "         1.0       0.68      0.71      0.69       156\n",
      "\n",
      "    accuracy                           0.67       298\n",
      "   macro avg       0.67      0.67      0.67       298\n",
      "weighted avg       0.67      0.67      0.67       298\n",
      "\n",
      "Test Accuracy: 0.6711\n",
      "Confusion Matrix:\n",
      "[[ 90  52]\n",
      " [ 46 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.63      0.65       142\n",
      "         1.0       0.68      0.71      0.69       156\n",
      "\n",
      "    accuracy                           0.67       298\n",
      "   macro avg       0.67      0.67      0.67       298\n",
      "weighted avg       0.67      0.67      0.67       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Avg. Loss: 0.6846\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488]\n",
      "Test Accuracy: 0.7785\n",
      "Confusion Matrix:\n",
      "[[101  41]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.71      0.75       142\n",
      "         1.0       0.76      0.84      0.80       156\n",
      "\n",
      "    accuracy                           0.78       298\n",
      "   macro avg       0.78      0.78      0.78       298\n",
      "weighted avg       0.78      0.78      0.78       298\n",
      "\n",
      "Test Accuracy: 0.7785\n",
      "Confusion Matrix:\n",
      "[[101  41]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.71      0.75       142\n",
      "         1.0       0.76      0.84      0.80       156\n",
      "\n",
      "    accuracy                           0.78       298\n",
      "   macro avg       0.78      0.78      0.78       298\n",
      "weighted avg       0.78      0.78      0.78       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg. Loss: 0.6676\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419]\n",
      "Test Accuracy: 0.8121\n",
      "Confusion Matrix:\n",
      "[[107  35]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.75      0.79       142\n",
      "         1.0       0.79      0.87      0.83       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.82      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Test Accuracy: 0.8121\n",
      "Confusion Matrix:\n",
      "[[107  35]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.75      0.79       142\n",
      "         1.0       0.79      0.87      0.83       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.82      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Avg. Loss: 0.6215\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738]\n",
      "Test Accuracy: 0.8087\n",
      "Confusion Matrix:\n",
      "[[103  39]\n",
      " [ 18 138]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.73      0.78       142\n",
      "         1.0       0.78      0.88      0.83       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.82      0.80      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Test Accuracy: 0.8087\n",
      "Confusion Matrix:\n",
      "[[103  39]\n",
      " [ 18 138]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.73      0.78       142\n",
      "         1.0       0.78      0.88      0.83       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.82      0.80      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Avg. Loss: 0.5350\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692]\n",
      "Test Accuracy: 0.8054\n",
      "Confusion Matrix:\n",
      "[[102  40]\n",
      " [ 18 138]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.72      0.78       142\n",
      "         1.0       0.78      0.88      0.83       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.80      0.80       298\n",
      "weighted avg       0.81      0.81      0.80       298\n",
      "\n",
      "Test Accuracy: 0.8054\n",
      "Confusion Matrix:\n",
      "[[102  40]\n",
      " [ 18 138]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.72      0.78       142\n",
      "         1.0       0.78      0.88      0.83       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.80      0.80       298\n",
      "weighted avg       0.81      0.81      0.80       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Avg. Loss: 0.4430\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[110  32]\n",
      " [ 20 136]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.77      0.81       142\n",
      "         1.0       0.81      0.87      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[110  32]\n",
      " [ 20 136]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.77      0.81       142\n",
      "         1.0       0.81      0.87      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.82       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Avg. Loss: 0.4432\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145]\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[114  28]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       142\n",
      "         1.0       0.83      0.87      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.83      0.83       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[114  28]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       142\n",
      "         1.0       0.83      0.87      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.83      0.83       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Avg. Loss: 0.4679\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[113  29]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       142\n",
      "         1.0       0.82      0.87      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[113  29]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       142\n",
      "         1.0       0.82      0.87      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Avg. Loss: 0.5160\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[113  29]\n",
      " [ 23 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.80      0.81       142\n",
      "         1.0       0.82      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[113  29]\n",
      " [ 23 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.80      0.81       142\n",
      "         1.0       0.82      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Avg. Loss: 0.5375\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247]\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 29 127]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.82      0.81       142\n",
      "         1.0       0.84      0.81      0.82       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 29 127]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.82      0.81       142\n",
      "         1.0       0.84      0.81      0.82       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Avg. Loss: 0.5558\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115]\n",
      "Test Accuracy: 0.8221\n",
      "Confusion Matrix:\n",
      "[[114  28]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       142\n",
      "         1.0       0.82      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8221\n",
      "Confusion Matrix:\n",
      "[[114  28]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       142\n",
      "         1.0       0.82      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Avg. Loss: 0.5943\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822]\n",
      "Test Accuracy: 0.8221\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8221\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114, 0.8221476510067114]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Avg. Loss: 0.7066\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114, 0.8221476510067114, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Avg. Loss: 0.6489\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.82       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.82       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114, 0.8221476510067114, 0.8322147651006712, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Avg. Loss: 0.7137\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       142\n",
      "         1.0       0.83      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       142\n",
      "         1.0       0.83      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114, 0.8221476510067114, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Avg. Loss: 0.6706\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[113  29]\n",
      " [ 22 134]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       142\n",
      "         1.0       0.82      0.86      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[113  29]\n",
      " [ 22 134]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       142\n",
      "         1.0       0.82      0.86      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114, 0.8221476510067114, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Avg. Loss: 0.6791\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       142\n",
      "         1.0       0.83      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       142\n",
      "         1.0       0.83      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114, 0.8221476510067114, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Avg. Loss: 0.6371\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.82       142\n",
      "         1.0       0.83      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.82       142\n",
      "         1.0       0.83      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114, 0.8221476510067114, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Avg. Loss: 0.7182\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[113  29]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       142\n",
      "         1.0       0.82      0.87      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[113  29]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       142\n",
      "         1.0       0.82      0.87      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114, 0.8221476510067114, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.825503355704698, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Avg. Loss: 0.6658\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603]\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.81      0.83       142\n",
      "         1.0       0.83      0.87      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.81      0.83       142\n",
      "         1.0       0.83      0.87      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114, 0.8221476510067114, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Avg. Loss: 0.7168\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603, 9.366998192031954]\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 23 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 23 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114, 0.8221476510067114, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Avg. Loss: 0.6664\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603, 9.366998192031954, 9.527456459003078]\n",
      "Test Accuracy: 0.8456\n",
      "Confusion Matrix:\n",
      "[[114  28]\n",
      " [ 18 138]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.80      0.83       142\n",
      "         1.0       0.83      0.88      0.86       156\n",
      "\n",
      "    accuracy                           0.85       298\n",
      "   macro avg       0.85      0.84      0.84       298\n",
      "weighted avg       0.85      0.85      0.85       298\n",
      "\n",
      "Test Accuracy: 0.8456\n",
      "Confusion Matrix:\n",
      "[[114  28]\n",
      " [ 18 138]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.80      0.83       142\n",
      "         1.0       0.83      0.88      0.86       156\n",
      "\n",
      "    accuracy                           0.85       298\n",
      "   macro avg       0.85      0.84      0.84       298\n",
      "weighted avg       0.85      0.85      0.85       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114, 0.8221476510067114, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.8456375838926175]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Avg. Loss: 0.6602\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603, 9.366998192031954, 9.527456459003078, 9.685367675387365]\n",
      "Test Accuracy: 0.8456\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.82      0.84       142\n",
      "         1.0       0.84      0.87      0.85       156\n",
      "\n",
      "    accuracy                           0.85       298\n",
      "   macro avg       0.85      0.84      0.85       298\n",
      "weighted avg       0.85      0.85      0.85       298\n",
      "\n",
      "Test Accuracy: 0.8456\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.82      0.84       142\n",
      "         1.0       0.84      0.87      0.85       156\n",
      "\n",
      "    accuracy                           0.85       298\n",
      "   macro avg       0.85      0.84      0.85       298\n",
      "weighted avg       0.85      0.85      0.85       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114, 0.8221476510067114, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.8456375838926175, 0.8456375838926175]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Avg. Loss: 0.6746\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603, 9.366998192031954, 9.527456459003078, 9.685367675387365, 9.840875662342482]\n",
      "Test Accuracy: 0.8490\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 22 134]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.84      0.84       142\n",
      "         1.0       0.85      0.86      0.86       156\n",
      "\n",
      "    accuracy                           0.85       298\n",
      "   macro avg       0.85      0.85      0.85       298\n",
      "weighted avg       0.85      0.85      0.85       298\n",
      "\n",
      "Test Accuracy: 0.8490\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 22 134]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.84      0.84       142\n",
      "         1.0       0.85      0.86      0.86       156\n",
      "\n",
      "    accuracy                           0.85       298\n",
      "   macro avg       0.85      0.85      0.85       298\n",
      "weighted avg       0.85      0.85      0.85       298\n",
      "\n",
      "Accuracy list is  [0.6711409395973155, 0.7785234899328859, 0.8120805369127517, 0.8087248322147651, 0.8053691275167785, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8187919463087249, 0.8221476510067114, 0.8221476510067114, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.8456375838926175, 0.8456375838926175, 0.8489932885906041]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg. Loss: 0.6914\n",
      "epsilon list is  [3.645415621190708]\n",
      "Test Accuracy: 0.4799\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [155   1]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       1.00      0.01      0.01       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.74      0.50      0.33       298\n",
      "weighted avg       0.75      0.48      0.31       298\n",
      "\n",
      "Test Accuracy: 0.4799\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [155   1]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       1.00      0.01      0.01       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.74      0.50      0.33       298\n",
      "weighted avg       0.75      0.48      0.31       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Avg. Loss: 0.6781\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615]\n",
      "Test Accuracy: 0.7953\n",
      "Confusion Matrix:\n",
      "[[132  10]\n",
      " [ 51 105]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.93      0.81       142\n",
      "         1.0       0.91      0.67      0.77       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.82      0.80      0.79       298\n",
      "weighted avg       0.82      0.80      0.79       298\n",
      "\n",
      "Test Accuracy: 0.7953\n",
      "Confusion Matrix:\n",
      "[[132  10]\n",
      " [ 51 105]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.93      0.81       142\n",
      "         1.0       0.91      0.67      0.77       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.82      0.80      0.79       298\n",
      "weighted avg       0.82      0.80      0.79       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg. Loss: 0.6481\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136]\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Avg. Loss: 0.5902\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[104  38]\n",
      " [ 14 142]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.73      0.80       142\n",
      "         1.0       0.79      0.91      0.85       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.84      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[104  38]\n",
      " [ 14 142]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.73      0.80       142\n",
      "         1.0       0.79      0.91      0.85       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.84      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.82       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Avg. Loss: 0.5038\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[106  36]\n",
      " [ 16 140]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.75      0.80       142\n",
      "         1.0       0.80      0.90      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[106  36]\n",
      " [ 16 140]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.75      0.80       142\n",
      "         1.0       0.80      0.90      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.82       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Avg. Loss: 0.4518\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[111  31]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.78      0.81       142\n",
      "         1.0       0.81      0.87      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[111  31]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.78      0.81       142\n",
      "         1.0       0.81      0.87      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Avg. Loss: 0.4407\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466]\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Avg. Loss: 0.4634\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478]\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       142\n",
      "         1.0       0.85      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       142\n",
      "         1.0       0.85      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Avg. Loss: 0.5368\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Avg. Loss: 0.5181\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053]\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.84      0.83       142\n",
      "         1.0       0.85      0.84      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.84      0.83       142\n",
      "         1.0       0.85      0.84      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Avg. Loss: 0.5468\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659]\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       142\n",
      "         1.0       0.85      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       142\n",
      "         1.0       0.85      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Avg. Loss: 0.5347\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.83      0.83       142\n",
      "         1.0       0.84      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.83      0.83       142\n",
      "         1.0       0.84      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Avg. Loss: 0.6062\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337]\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.84      0.83       142\n",
      "         1.0       0.85      0.84      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.84      0.83       142\n",
      "         1.0       0.85      0.84      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Avg. Loss: 0.6030\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224]\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       142\n",
      "         1.0       0.85      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       142\n",
      "         1.0       0.85      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Avg. Loss: 0.6022\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569]\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.84      0.83       142\n",
      "         1.0       0.85      0.84      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.84      0.83       142\n",
      "         1.0       0.85      0.84      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Avg. Loss: 0.6194\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       142\n",
      "         1.0       0.85      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       142\n",
      "         1.0       0.85      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Avg. Loss: 0.6424\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574]\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[120  22]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.85      0.83       142\n",
      "         1.0       0.86      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[120  22]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.85      0.83       142\n",
      "         1.0       0.86      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Avg. Loss: 0.6391\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       142\n",
      "         1.0       0.85      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       142\n",
      "         1.0       0.85      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Avg. Loss: 0.7471\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724]\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.83      0.83       142\n",
      "         1.0       0.85      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.83      0.83       142\n",
      "         1.0       0.85      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Avg. Loss: 0.6474\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[120  22]\n",
      " [ 28 128]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.85      0.83       142\n",
      "         1.0       0.85      0.82      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[120  22]\n",
      " [ 28 128]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.85      0.83       142\n",
      "         1.0       0.85      0.82      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Avg. Loss: 0.7089\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       142\n",
      "         1.0       0.85      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       142\n",
      "         1.0       0.85      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Avg. Loss: 0.8340\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.83      0.82       142\n",
      "         1.0       0.84      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.83      0.82       142\n",
      "         1.0       0.84      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Avg. Loss: 0.7585\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Avg. Loss: 0.7939\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.83      0.82       142\n",
      "         1.0       0.84      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.83      0.82       142\n",
      "         1.0       0.84      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Avg. Loss: 0.6981\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Avg. Loss: 0.7998\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.83      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.83      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Avg. Loss: 0.8018\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Avg. Loss: 0.8260\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 23 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       142\n",
      "         1.0       0.83      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 23 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       142\n",
      "         1.0       0.83      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Avg. Loss: 0.6691\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.83      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.83      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Avg. Loss: 0.7600\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.83      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.83      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Avg. Loss: 0.6750\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675]\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Avg. Loss: 0.6884\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       142\n",
      "         1.0       0.83      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       142\n",
      "         1.0       0.83      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Avg. Loss: 0.8353\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.82       142\n",
      "         1.0       0.83      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.82       142\n",
      "         1.0       0.83      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Avg. Loss: 0.7412\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.82       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.82       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Avg. Loss: 0.8024\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339]\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 23 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 23 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Avg. Loss: 0.7936\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168]\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 23 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 23 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Avg. Loss: 0.8009\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168, 8.765939096947427]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.82       142\n",
      "         1.0       0.83      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.82       142\n",
      "         1.0       0.83      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Avg. Loss: 0.9591\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168, 8.765939096947427, 8.865091432753177]\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.83      0.83       142\n",
      "         1.0       0.85      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.83      0.83       142\n",
      "         1.0       0.85      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.825503355704698, 0.8355704697986577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Avg. Loss: 0.6936\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168, 8.765939096947427, 8.865091432753177, 8.963384242673605]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 23 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       142\n",
      "         1.0       0.83      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 23 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       142\n",
      "         1.0       0.83      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.825503355704698, 0.8355704697986577, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Avg. Loss: 0.8432\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168, 8.765939096947427, 8.865091432753177, 8.963384242673605, 9.06083997833836]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.82       142\n",
      "         1.0       0.83      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.82       142\n",
      "         1.0       0.83      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.82      0.82       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Avg. Loss: 0.9834\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168, 8.765939096947427, 8.865091432753177, 8.963384242673605, 9.06083997833836, 9.157495763201062]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Avg. Loss: 0.8976\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168, 8.765939096947427, 8.865091432753177, 8.963384242673605, 9.06083997833836, 9.157495763201062, 9.253373208251261]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 28 128]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.84      0.82       142\n",
      "         1.0       0.85      0.82      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 28 128]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.84      0.82       142\n",
      "         1.0       0.85      0.82      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8288590604026845, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Avg. Loss: 0.8040\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168, 8.765939096947427, 8.865091432753177, 8.963384242673605, 9.06083997833836, 9.157495763201062, 9.253373208251261, 9.348486513977836]\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Avg. Loss: 0.7103\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168, 8.765939096947427, 8.865091432753177, 8.963384242673605, 9.06083997833836, 9.157495763201062, 9.253373208251261, 9.348486513977836, 9.442878287740127]\n",
      "Test Accuracy: 0.8221\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 30 126]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.84      0.82       142\n",
      "         1.0       0.85      0.81      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8221\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 30 126]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.84      0.82       142\n",
      "         1.0       0.85      0.81      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Avg. Loss: 0.9302\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168, 8.765939096947427, 8.865091432753177, 8.963384242673605, 9.06083997833836, 9.157495763201062, 9.253373208251261, 9.348486513977836, 9.442878287740127, 9.536556482868438]\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 28 128]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.81       142\n",
      "         1.0       0.83      0.82      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 28 128]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.81       142\n",
      "         1.0       0.83      0.82      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114, 0.8187919463087249]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Avg. Loss: 0.8435\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168, 8.765939096947427, 8.865091432753177, 8.963384242673605, 9.06083997833836, 9.157495763201062, 9.253373208251261, 9.348486513977836, 9.442878287740127, 9.536556482868438, 9.629543931072343]\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[120  22]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.85      0.83       142\n",
      "         1.0       0.86      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[120  22]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.85      0.83       142\n",
      "         1.0       0.86      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114, 0.8187919463087249, 0.8389261744966443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Avg. Loss: 1.0315\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168, 8.765939096947427, 8.865091432753177, 8.963384242673605, 9.06083997833836, 9.157495763201062, 9.253373208251261, 9.348486513977836, 9.442878287740127, 9.536556482868438, 9.629543931072343, 9.721862489732121]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 29 127]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.84      0.82       142\n",
      "         1.0       0.85      0.81      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 29 127]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.84      0.82       142\n",
      "         1.0       0.85      0.81      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114, 0.8187919463087249, 0.8389261744966443, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Avg. Loss: 1.0098\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168, 8.765939096947427, 8.865091432753177, 8.963384242673605, 9.06083997833836, 9.157495763201062, 9.253373208251261, 9.348486513977836, 9.442878287740127, 9.536556482868438, 9.629543931072343, 9.721862489732121, 9.813528347984592]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 29 127]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.84      0.82       142\n",
      "         1.0       0.85      0.81      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[119  23]\n",
      " [ 29 127]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.84      0.82       142\n",
      "         1.0       0.85      0.81      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114, 0.8187919463087249, 0.8389261744966443, 0.825503355704698, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Avg. Loss: 0.9212\n",
      "epsilon list is  [3.645415621190708, 3.997446746718615, 4.263094997728136, 4.491112102907805, 4.696817684532357, 4.887218880404754, 5.0662030736519466, 5.236191346694478, 5.398837945326178, 5.55531918263053, 5.706514132626659, 5.853108008100817, 5.995634582061337, 6.134531762646224, 6.270158169059569, 6.402816667251692, 6.532756510409574, 6.6601939652482045, 6.78531549274724, 6.908293498298983, 7.029260941617798, 7.1483524508429, 7.265677732655726, 7.381335688811612, 7.495425150946855, 7.608025731546569, 7.719210704085181, 7.829046481422687, 7.937602196178515, 8.04493031399542, 8.151084819930675, 8.256119985340703, 8.360077058650386, 8.462998301701857, 8.564922301165339, 8.665892636683168, 8.765939096947427, 8.865091432753177, 8.963384242673605, 9.06083997833836, 9.157495763201062, 9.253373208251261, 9.348486513977836, 9.442878287740127, 9.536556482868438, 9.629543931072343, 9.721862489732121, 9.813528347984592, 9.90456516592753]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.83      0.83       142\n",
      "         1.0       0.84      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.83      0.83       142\n",
      "         1.0       0.84      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.4798657718120805, 0.7953020134228188, 0.8187919463087249, 0.825503355704698, 0.825503355704698, 0.825503355704698, 0.8355704697986577, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8322147651006712, 0.8389261744966443, 0.8355704697986577, 0.8389261744966443, 0.8322147651006712, 0.8389261744966443, 0.8322147651006712, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8288590604026845, 0.8322147651006712, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8288590604026845, 0.825503355704698, 0.8322147651006712, 0.8389261744966443, 0.8389261744966443, 0.825503355704698, 0.8355704697986577, 0.8322147651006712, 0.825503355704698, 0.8288590604026845, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114, 0.8187919463087249, 0.8389261744966443, 0.825503355704698, 0.825503355704698, 0.8322147651006712]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from opacus import PrivacyEngine\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "##Making the model \n",
    "class HeartDiseaseModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HeartDiseaseModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "## Added 5 layers with a dropout layer\n",
    "input_size = 30  # number of features\n",
    "\n",
    "model = HeartDiseaseModel(input_size)\n",
    "\n",
    "cleveland = pd.read_csv('C:/Users/siddh/standardized_data.csv')\n",
    "print('Shape of DataFrame: {}'.format(cleveland.shape))\n",
    "print(cleveland.loc[1])\n",
    "\n",
    "cleveland.head()\n",
    "\n",
    "data = cleveland[~cleveland.isin(['?'])]\n",
    "data.loc[280:]\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "# renaming columns\n",
    "\n",
    "# dealing with categorical variables for better inference with the model\n",
    "def convert_encoding(data):\n",
    "    dummies=pd.get_dummies(data['sex'],prefix='sex')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('sex',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['slope'],prefix='slope')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('slope',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['exang'],prefix='exang')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('exang',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['restecg'],prefix='restecg')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('restecg',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['cp'],prefix='cp')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('cp',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['fbs'],prefix='fbs')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('fbs',axis=1,inplace=True)\n",
    "    \n",
    "    dummies=pd.get_dummies(data['ca'],prefix='ca')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('ca',axis=1,inplace=True)\n",
    "    \n",
    "    dummies=pd.get_dummies(data['thal'],prefix='thal')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('thal',axis=1,inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data=convert_encoding(data)\n",
    "\n",
    "print(data.shape)\n",
    "## Splitting data to fit into our model with standardization\n",
    "y = data['num']\n",
    "X = data.drop(['num'], axis=1)\n",
    "y = y.to_numpy()\n",
    "X = X.to_numpy()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    return torch.from_numpy(df).float()\n",
    "\n",
    "X_traint = df_to_tensor(X_train)\n",
    "y_traint = df_to_tensor(y_train)\n",
    "X_testt = df_to_tensor(X_test)\n",
    "y_testt = df_to_tensor(y_test)\n",
    "\n",
    "train_ds = TensorDataset(X_traint, y_traint)\n",
    "test_ds = TensorDataset(X_testt, y_testt)\n",
    "\n",
    "# create data loaders\n",
    "batch_size = 5\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "delta=1e-7\n",
    "## Privacy engine object is used to call the opacus privacy engine class\n",
    "privacy_engine = PrivacyEngine()\n",
    "loss_fn = nn.BCELoss() # Binary Cross Entropy loss for binary losses\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "model, optimizer, dataloader = privacy_engine.make_private_with_epsilon(module=model,optimizer=optimizer,data_loader=train_dataloader,\n",
    "                                                         max_grad_norm=1.0,target_epsilon=10, epochs=10, target_delta= 1e-7)\n",
    "criterion = nn.BCELoss()\n",
    "epsilon_values = []\n",
    "\n",
    "def train(model, train_dataloader, optimizer, epoch, epsilon_values):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        ##reorganizing the output to compare to the output from the file\n",
    "        target = target.unsqueeze(1).float()\n",
    "        target = target.repeat(1, output.shape[1])\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    epsilon = float(privacy_engine.get_epsilon(delta))\n",
    "    epsilon_values.append(epsilon)\n",
    "    print('Epoch: {}, Avg. Loss: {:.4f}'.format(epoch, np.mean(losses)))\n",
    "    return epsilon_values\n",
    "\n",
    "def test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            output = model(data)\n",
    "            predicted = torch.round(output)\n",
    "            predictions.extend(predicted.tolist()) ##converting the output to a list for accuracy check\n",
    "            targets.extend(target.tolist())\n",
    "    acc = accuracy_score(targets, predictions)\n",
    "    print('Test Accuracy: {:.4f}'.format(acc))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(targets, predictions))\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(targets, predictions))\n",
    "    return acc\n",
    "\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "x=[10,25,50]\n",
    "for i,j in enumerate(x):\n",
    "    accuracies = []\n",
    "    epsilon_list=[]\n",
    "    epsilon_values = []\n",
    "    epsilon_list_total=[]\n",
    "    model1 = HeartDiseaseModel(input_size)\n",
    "    model2 = HeartDiseaseModel(input_size)\n",
    "    model3 = HeartDiseaseModel(input_size)\n",
    "    if(i==0):\n",
    "        model=model1\n",
    "    elif (i==1):\n",
    "        model=model2\n",
    "    else:\n",
    "        model=model3\n",
    "        \n",
    "    privacy_engine = PrivacyEngine()\n",
    "    loss_fn = nn.BCELoss() # Binary Cross Entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model, optimizer, dataloader = privacy_engine.make_private_with_epsilon(module=model,optimizer=optimizer,data_loader=train_dataloader,\n",
    "                                                         max_grad_norm=1.0,target_epsilon=10, epochs=j, target_delta= 1e-7)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(1,j):\n",
    "        epsilon_list=train(model, train_dataloader, optimizer, epoch, epsilon_values)\n",
    "        print('epsilon list is ', epsilon_list)\n",
    "        test(model, test_dataloader)\n",
    "        acc = test(model, test_dataloader)\n",
    "        accuracies.append(acc)\n",
    "        print('Accuracy list is ',accuracies)\n",
    "    \n",
    "    if (i==0):\n",
    "        accuracies=[i*100 for i in accuracies]\n",
    "        plt.plot(epsilon_list, accuracies,color=\"red\",label='10 epochs')\n",
    "    elif(i==1):\n",
    "        accuracies=[i*100 for i in accuracies]\n",
    "        plt.plot(epsilon_list, accuracies,color=\"green\", label='25 epochs')\n",
    "    else:\n",
    "        accuracies=[i*100 for i in accuracies] \n",
    "        plt.plot(epsilon_list, accuracies, color=\"blue\", label='50 epochs')    \n",
    "    del model\n",
    "\n",
    "plt.xticks(range(0,10)) \n",
    "plt.yticks(range(int(accuracies[0]),100,3)) \n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Change in Accuracy with Epsilon')\n",
    "plt.legend(loc ='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# for x,y in zip(epsilon_list,accuracies):\n",
    "#     count = 0\n",
    "#     label = 'epoch'+ str(count)\n",
    "#     if(count%4==0):\n",
    "#         plt.annotate(label,(x,y),textcoords='offset points',xytext=(0,10),ha='center')\n",
    "#     else:\n",
    "#         continue\n",
    "#     count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48a633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: (2110, 14)\n",
      "age        -0.493310\n",
      "sex        -1.855291\n",
      "cp         -0.257508\n",
      "trestbps    1.493132\n",
      "chol       -0.241968\n",
      "fbs        -0.490700\n",
      "restecg    -0.779254\n",
      "thalch      0.668772\n",
      "exang      -0.796772\n",
      "oldpeak     0.087912\n",
      "slope       0.528227\n",
      "ca         -0.276244\n",
      "thal       -0.528078\n",
      "num         1.000000\n",
      "Name: 1, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg. Loss: 0.6921\n",
      "epsilon list is  [4.673786299411615]\n",
      "Test Accuracy: 0.6040\n",
      "Confusion Matrix:\n",
      "[[ 28 114]\n",
      " [  4 152]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.20      0.32       142\n",
      "         1.0       0.57      0.97      0.72       156\n",
      "\n",
      "    accuracy                           0.60       298\n",
      "   macro avg       0.72      0.59      0.52       298\n",
      "weighted avg       0.72      0.60      0.53       298\n",
      "\n",
      "Test Accuracy: 0.6040\n",
      "Confusion Matrix:\n",
      "[[ 28 114]\n",
      " [  4 152]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.20      0.32       142\n",
      "         1.0       0.57      0.97      0.72       156\n",
      "\n",
      "    accuracy                           0.60       298\n",
      "   macro avg       0.72      0.59      0.52       298\n",
      "weighted avg       0.72      0.60      0.53       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Avg. Loss: 0.6860\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488]\n",
      "Test Accuracy: 0.6812\n",
      "Confusion Matrix:\n",
      "[[ 50  92]\n",
      " [  3 153]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.35      0.51       142\n",
      "         1.0       0.62      0.98      0.76       156\n",
      "\n",
      "    accuracy                           0.68       298\n",
      "   macro avg       0.78      0.67      0.64       298\n",
      "weighted avg       0.78      0.68      0.64       298\n",
      "\n",
      "Test Accuracy: 0.6812\n",
      "Confusion Matrix:\n",
      "[[ 50  92]\n",
      " [  3 153]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.35      0.51       142\n",
      "         1.0       0.62      0.98      0.76       156\n",
      "\n",
      "    accuracy                           0.68       298\n",
      "   macro avg       0.78      0.67      0.64       298\n",
      "weighted avg       0.78      0.68      0.64       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg. Loss: 0.6759\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419]\n",
      "Test Accuracy: 0.7651\n",
      "Confusion Matrix:\n",
      "[[ 80  62]\n",
      " [  8 148]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.56      0.70       142\n",
      "         1.0       0.70      0.95      0.81       156\n",
      "\n",
      "    accuracy                           0.77       298\n",
      "   macro avg       0.81      0.76      0.75       298\n",
      "weighted avg       0.80      0.77      0.75       298\n",
      "\n",
      "Test Accuracy: 0.7651\n",
      "Confusion Matrix:\n",
      "[[ 80  62]\n",
      " [  8 148]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.56      0.70       142\n",
      "         1.0       0.70      0.95      0.81       156\n",
      "\n",
      "    accuracy                           0.77       298\n",
      "   macro avg       0.81      0.76      0.75       298\n",
      "weighted avg       0.80      0.77      0.75       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Avg. Loss: 0.6367\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738]\n",
      "Test Accuracy: 0.8020\n",
      "Confusion Matrix:\n",
      "[[ 93  49]\n",
      " [ 10 146]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.65      0.76       142\n",
      "         1.0       0.75      0.94      0.83       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.83      0.80      0.80       298\n",
      "weighted avg       0.82      0.80      0.80       298\n",
      "\n",
      "Test Accuracy: 0.8020\n",
      "Confusion Matrix:\n",
      "[[ 93  49]\n",
      " [ 10 146]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.65      0.76       142\n",
      "         1.0       0.75      0.94      0.83       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.83      0.80      0.80       298\n",
      "weighted avg       0.82      0.80      0.80       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Avg. Loss: 0.5647\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692]\n",
      "Test Accuracy: 0.8020\n",
      "Confusion Matrix:\n",
      "[[ 97  45]\n",
      " [ 14 142]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.68      0.77       142\n",
      "         1.0       0.76      0.91      0.83       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.82      0.80      0.80       298\n",
      "weighted avg       0.81      0.80      0.80       298\n",
      "\n",
      "Test Accuracy: 0.8020\n",
      "Confusion Matrix:\n",
      "[[ 97  45]\n",
      " [ 14 142]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.68      0.77       142\n",
      "         1.0       0.76      0.91      0.83       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.82      0.80      0.80       298\n",
      "weighted avg       0.81      0.80      0.80       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Avg. Loss: 0.4733\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531]\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[103  39]\n",
      " [ 15 141]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.73      0.79       142\n",
      "         1.0       0.78      0.90      0.84       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.83      0.81      0.82       298\n",
      "weighted avg       0.83      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[103  39]\n",
      " [ 15 141]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.73      0.79       142\n",
      "         1.0       0.78      0.90      0.84       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.83      0.81      0.82       298\n",
      "weighted avg       0.83      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Avg. Loss: 0.4377\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145]\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[114  28]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       142\n",
      "         1.0       0.83      0.87      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.83      0.83       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[114  28]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       142\n",
      "         1.0       0.83      0.87      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.83      0.83       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Avg. Loss: 0.4510\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[111  31]\n",
      " [ 19 137]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.78      0.82       142\n",
      "         1.0       0.82      0.88      0.85       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[111  31]\n",
      " [ 19 137]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.78      0.82       142\n",
      "         1.0       0.82      0.88      0.85       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Avg. Loss: 0.5178\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[111  31]\n",
      " [ 19 137]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.78      0.82       142\n",
      "         1.0       0.82      0.88      0.85       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[111  31]\n",
      " [ 19 137]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.78      0.82       142\n",
      "         1.0       0.82      0.88      0.85       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Avg. Loss: 0.5087\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247]\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.83      0.83       142\n",
      "         1.0       0.85      0.85      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.83      0.83       142\n",
      "         1.0       0.85      0.85      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Avg. Loss: 0.5682\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115]\n",
      "Test Accuracy: 0.8456\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 20 136]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.82      0.83       142\n",
      "         1.0       0.84      0.87      0.86       156\n",
      "\n",
      "    accuracy                           0.85       298\n",
      "   macro avg       0.85      0.84      0.84       298\n",
      "weighted avg       0.85      0.85      0.85       298\n",
      "\n",
      "Test Accuracy: 0.8456\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 20 136]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.82      0.83       142\n",
      "         1.0       0.84      0.87      0.86       156\n",
      "\n",
      "    accuracy                           0.85       298\n",
      "   macro avg       0.85      0.84      0.84       298\n",
      "weighted avg       0.85      0.85      0.85       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Avg. Loss: 0.5434\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822]\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.81      0.83       142\n",
      "         1.0       0.83      0.87      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8389\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 21 135]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.81      0.83       142\n",
      "         1.0       0.83      0.87      0.85       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175, 0.8389261744966443]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Avg. Loss: 0.5710\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406]\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175, 0.8389261744966443, 0.8355704697986577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Avg. Loss: 0.6141\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838]\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175, 0.8389261744966443, 0.8355704697986577, 0.8355704697986577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Avg. Loss: 0.6529\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.83      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.83      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175, 0.8389261744966443, 0.8355704697986577, 0.8355704697986577, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Avg. Loss: 0.6715\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267]\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175, 0.8389261744966443, 0.8355704697986577, 0.8355704697986577, 0.8288590604026845, 0.8187919463087249]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Avg. Loss: 0.6811\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156]\n",
      "Test Accuracy: 0.8221\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8221\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175, 0.8389261744966443, 0.8355704697986577, 0.8355704697986577, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Avg. Loss: 0.6659\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.82       142\n",
      "         1.0       0.84      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.82       142\n",
      "         1.0       0.84      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175, 0.8389261744966443, 0.8355704697986577, 0.8355704697986577, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Avg. Loss: 0.6412\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444]\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8255\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175, 0.8389261744966443, 0.8355704697986577, 0.8355704697986577, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114, 0.825503355704698, 0.825503355704698]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Avg. Loss: 0.7137\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.83      0.83       142\n",
      "         1.0       0.84      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[118  24]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.83      0.83       142\n",
      "         1.0       0.84      0.83      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175, 0.8389261744966443, 0.8355704697986577, 0.8355704697986577, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114, 0.825503355704698, 0.825503355704698, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Avg. Loss: 0.6480\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603, 9.366998192031954]\n",
      "Test Accuracy: 0.8221\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8221\n",
      "Confusion Matrix:\n",
      "[[116  26]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.81       142\n",
      "         1.0       0.83      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175, 0.8389261744966443, 0.8355704697986577, 0.8355704697986577, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114, 0.825503355704698, 0.825503355704698, 0.8322147651006712, 0.8221476510067114]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Avg. Loss: 0.6406\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603, 9.366998192031954, 9.527456459003078]\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Test Accuracy: 0.8356\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       142\n",
      "         1.0       0.84      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.84       298\n",
      "   macro avg       0.84      0.84      0.84       298\n",
      "weighted avg       0.84      0.84      0.84       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175, 0.8389261744966443, 0.8355704697986577, 0.8355704697986577, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114, 0.825503355704698, 0.825503355704698, 0.8322147651006712, 0.8221476510067114, 0.8355704697986577]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Avg. Loss: 0.6434\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603, 9.366998192031954, 9.527456459003078, 9.685367675387365]\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       142\n",
      "         1.0       0.83      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8289\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       142\n",
      "         1.0       0.83      0.85      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175, 0.8389261744966443, 0.8355704697986577, 0.8355704697986577, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114, 0.825503355704698, 0.825503355704698, 0.8322147651006712, 0.8221476510067114, 0.8355704697986577, 0.8288590604026845]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Avg. Loss: 0.6420\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603, 9.366998192031954, 9.527456459003078, 9.685367675387365, 9.840875662342482]\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Test Accuracy: 0.8322\n",
      "Confusion Matrix:\n",
      "[[117  25]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       142\n",
      "         1.0       0.84      0.84      0.84       156\n",
      "\n",
      "    accuracy                           0.83       298\n",
      "   macro avg       0.83      0.83      0.83       298\n",
      "weighted avg       0.83      0.83      0.83       298\n",
      "\n",
      "Accuracy list is  [0.6040268456375839, 0.6812080536912751, 0.7651006711409396, 0.802013422818792, 0.802013422818792, 0.8187919463087249, 0.8355704697986577, 0.8322147651006712, 0.8322147651006712, 0.8389261744966443, 0.8456375838926175, 0.8389261744966443, 0.8355704697986577, 0.8355704697986577, 0.8288590604026845, 0.8187919463087249, 0.8221476510067114, 0.825503355704698, 0.825503355704698, 0.8322147651006712, 0.8221476510067114, 0.8355704697986577, 0.8288590604026845, 0.8322147651006712]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg. Loss: 0.6959\n",
      "epsilon list is  [4.673786299411615]\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Avg. Loss: 0.6940\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488]\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg. Loss: 0.6926\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419]\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Avg. Loss: 0.6915\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738]\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Avg. Loss: 0.6903\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692]\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Avg. Loss: 0.6895\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531]\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Avg. Loss: 0.6874\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145]\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Test Accuracy: 0.4765\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [156   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.24      0.50      0.32       298\n",
      "weighted avg       0.23      0.48      0.31       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Avg. Loss: 0.6851\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618]\n",
      "Test Accuracy: 0.4799\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [155   1]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       1.00      0.01      0.01       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.74      0.50      0.33       298\n",
      "weighted avg       0.75      0.48      0.31       298\n",
      "\n",
      "Test Accuracy: 0.4799\n",
      "Confusion Matrix:\n",
      "[[142   0]\n",
      " [155   1]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      1.00      0.65       142\n",
      "         1.0       1.00      0.01      0.01       156\n",
      "\n",
      "    accuracy                           0.48       298\n",
      "   macro avg       0.74      0.50      0.33       298\n",
      "weighted avg       0.75      0.48      0.31       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Avg. Loss: 0.6839\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225]\n",
      "Test Accuracy: 0.5772\n",
      "Confusion Matrix:\n",
      "[[141   1]\n",
      " [125  31]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.99      0.69       142\n",
      "         1.0       0.97      0.20      0.33       156\n",
      "\n",
      "    accuracy                           0.58       298\n",
      "   macro avg       0.75      0.60      0.51       298\n",
      "weighted avg       0.76      0.58      0.50       298\n",
      "\n",
      "Test Accuracy: 0.5772\n",
      "Confusion Matrix:\n",
      "[[141   1]\n",
      " [125  31]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.99      0.69       142\n",
      "         1.0       0.97      0.20      0.33       156\n",
      "\n",
      "    accuracy                           0.58       298\n",
      "   macro avg       0.75      0.60      0.51       298\n",
      "weighted avg       0.76      0.58      0.50       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Avg. Loss: 0.6816\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247]\n",
      "Test Accuracy: 0.7349\n",
      "Confusion Matrix:\n",
      "[[138   4]\n",
      " [ 75  81]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.97      0.78       142\n",
      "         1.0       0.95      0.52      0.67       156\n",
      "\n",
      "    accuracy                           0.73       298\n",
      "   macro avg       0.80      0.75      0.72       298\n",
      "weighted avg       0.81      0.73      0.72       298\n",
      "\n",
      "Test Accuracy: 0.7349\n",
      "Confusion Matrix:\n",
      "[[138   4]\n",
      " [ 75  81]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.97      0.78       142\n",
      "         1.0       0.95      0.52      0.67       156\n",
      "\n",
      "    accuracy                           0.73       298\n",
      "   macro avg       0.80      0.75      0.72       298\n",
      "weighted avg       0.81      0.73      0.72       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Avg. Loss: 0.6779\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115]\n",
      "Test Accuracy: 0.7785\n",
      "Confusion Matrix:\n",
      "[[127  15]\n",
      " [ 51 105]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.89      0.79       142\n",
      "         1.0       0.88      0.67      0.76       156\n",
      "\n",
      "    accuracy                           0.78       298\n",
      "   macro avg       0.79      0.78      0.78       298\n",
      "weighted avg       0.80      0.78      0.78       298\n",
      "\n",
      "Test Accuracy: 0.7785\n",
      "Confusion Matrix:\n",
      "[[127  15]\n",
      " [ 51 105]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.89      0.79       142\n",
      "         1.0       0.88      0.67      0.76       156\n",
      "\n",
      "    accuracy                           0.78       298\n",
      "   macro avg       0.79      0.78      0.78       298\n",
      "weighted avg       0.80      0.78      0.78       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Avg. Loss: 0.6747\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822]\n",
      "Test Accuracy: 0.7919\n",
      "Confusion Matrix:\n",
      "[[124  18]\n",
      " [ 44 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.87      0.80       142\n",
      "         1.0       0.86      0.72      0.78       156\n",
      "\n",
      "    accuracy                           0.79       298\n",
      "   macro avg       0.80      0.80      0.79       298\n",
      "weighted avg       0.80      0.79      0.79       298\n",
      "\n",
      "Test Accuracy: 0.7919\n",
      "Confusion Matrix:\n",
      "[[124  18]\n",
      " [ 44 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.87      0.80       142\n",
      "         1.0       0.86      0.72      0.78       156\n",
      "\n",
      "    accuracy                           0.79       298\n",
      "   macro avg       0.80      0.80      0.79       298\n",
      "weighted avg       0.80      0.79      0.79       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859, 0.7919463087248322]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Avg. Loss: 0.6724\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406]\n",
      "Test Accuracy: 0.7953\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 34 122]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.81      0.79       142\n",
      "         1.0       0.82      0.78      0.80       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.80      0.80      0.80       298\n",
      "weighted avg       0.80      0.80      0.80       298\n",
      "\n",
      "Test Accuracy: 0.7953\n",
      "Confusion Matrix:\n",
      "[[115  27]\n",
      " [ 34 122]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.81      0.79       142\n",
      "         1.0       0.82      0.78      0.80       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.80      0.80      0.80       298\n",
      "weighted avg       0.80      0.80      0.80       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859, 0.7919463087248322, 0.7953020134228188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Avg. Loss: 0.6694\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838]\n",
      "Test Accuracy: 0.8020\n",
      "Confusion Matrix:\n",
      "[[112  30]\n",
      " [ 29 127]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.79      0.79       142\n",
      "         1.0       0.81      0.81      0.81       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.80      0.80      0.80       298\n",
      "weighted avg       0.80      0.80      0.80       298\n",
      "\n",
      "Test Accuracy: 0.8020\n",
      "Confusion Matrix:\n",
      "[[112  30]\n",
      " [ 29 127]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.79      0.79       142\n",
      "         1.0       0.81      0.81      0.81       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.80      0.80      0.80       298\n",
      "weighted avg       0.80      0.80      0.80       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859, 0.7919463087248322, 0.7953020134228188, 0.802013422818792]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Avg. Loss: 0.6643\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047]\n",
      "Test Accuracy: 0.8020\n",
      "Confusion Matrix:\n",
      "[[110  32]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.77      0.79       142\n",
      "         1.0       0.80      0.83      0.81       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.80      0.80      0.80       298\n",
      "weighted avg       0.80      0.80      0.80       298\n",
      "\n",
      "Test Accuracy: 0.8020\n",
      "Confusion Matrix:\n",
      "[[110  32]\n",
      " [ 27 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.77      0.79       142\n",
      "         1.0       0.80      0.83      0.81       156\n",
      "\n",
      "    accuracy                           0.80       298\n",
      "   macro avg       0.80      0.80      0.80       298\n",
      "weighted avg       0.80      0.80      0.80       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859, 0.7919463087248322, 0.7953020134228188, 0.802013422818792, 0.802013422818792]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Avg. Loss: 0.6607\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267]\n",
      "Test Accuracy: 0.8087\n",
      "Confusion Matrix:\n",
      "[[111  31]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.80       142\n",
      "         1.0       0.81      0.83      0.82       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Test Accuracy: 0.8087\n",
      "Confusion Matrix:\n",
      "[[111  31]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.80       142\n",
      "         1.0       0.81      0.83      0.82       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859, 0.7919463087248322, 0.7953020134228188, 0.802013422818792, 0.802013422818792, 0.8087248322147651]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Avg. Loss: 0.6572\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156]\n",
      "Test Accuracy: 0.8087\n",
      "Confusion Matrix:\n",
      "[[109  33]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.77      0.79       142\n",
      "         1.0       0.80      0.85      0.82       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Test Accuracy: 0.8087\n",
      "Confusion Matrix:\n",
      "[[109  33]\n",
      " [ 24 132]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.77      0.79       142\n",
      "         1.0       0.80      0.85      0.82       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859, 0.7919463087248322, 0.7953020134228188, 0.802013422818792, 0.802013422818792, 0.8087248322147651, 0.8087248322147651]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Avg. Loss: 0.6489\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521]\n",
      "Test Accuracy: 0.8087\n",
      "Confusion Matrix:\n",
      "[[110  32]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.77      0.79       142\n",
      "         1.0       0.80      0.84      0.82       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Test Accuracy: 0.8087\n",
      "Confusion Matrix:\n",
      "[[110  32]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.77      0.79       142\n",
      "         1.0       0.80      0.84      0.82       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859, 0.7919463087248322, 0.7953020134228188, 0.802013422818792, 0.802013422818792, 0.8087248322147651, 0.8087248322147651, 0.8087248322147651]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Avg. Loss: 0.6432\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444]\n",
      "Test Accuracy: 0.8087\n",
      "Confusion Matrix:\n",
      "[[110  32]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.77      0.79       142\n",
      "         1.0       0.80      0.84      0.82       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Test Accuracy: 0.8087\n",
      "Confusion Matrix:\n",
      "[[110  32]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.77      0.79       142\n",
      "         1.0       0.80      0.84      0.82       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859, 0.7919463087248322, 0.7953020134228188, 0.802013422818792, 0.802013422818792, 0.8087248322147651, 0.8087248322147651, 0.8087248322147651, 0.8087248322147651]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Avg. Loss: 0.6356\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603]\n",
      "Test Accuracy: 0.8121\n",
      "Confusion Matrix:\n",
      "[[111  31]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.78      0.80       142\n",
      "         1.0       0.81      0.84      0.82       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Test Accuracy: 0.8121\n",
      "Confusion Matrix:\n",
      "[[111  31]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.78      0.80       142\n",
      "         1.0       0.81      0.84      0.82       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859, 0.7919463087248322, 0.7953020134228188, 0.802013422818792, 0.802013422818792, 0.8087248322147651, 0.8087248322147651, 0.8087248322147651, 0.8087248322147651, 0.8120805369127517]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Avg. Loss: 0.6269\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603, 9.366998192031954]\n",
      "Test Accuracy: 0.8121\n",
      "Confusion Matrix:\n",
      "[[112  30]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.79      0.80       142\n",
      "         1.0       0.81      0.83      0.82       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Test Accuracy: 0.8121\n",
      "Confusion Matrix:\n",
      "[[112  30]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.79      0.80       142\n",
      "         1.0       0.81      0.83      0.82       156\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.81      0.81      0.81       298\n",
      "weighted avg       0.81      0.81      0.81       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859, 0.7919463087248322, 0.7953020134228188, 0.802013422818792, 0.802013422818792, 0.8087248322147651, 0.8087248322147651, 0.8087248322147651, 0.8087248322147651, 0.8120805369127517, 0.8120805369127517]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Avg. Loss: 0.6156\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603, 9.366998192031954, 9.527456459003078]\n",
      "Test Accuracy: 0.8154\n",
      "Confusion Matrix:\n",
      "[[112  30]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       142\n",
      "         1.0       0.81      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.81      0.81       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8154\n",
      "Confusion Matrix:\n",
      "[[112  30]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       142\n",
      "         1.0       0.81      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.81      0.81       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859, 0.7919463087248322, 0.7953020134228188, 0.802013422818792, 0.802013422818792, 0.8087248322147651, 0.8087248322147651, 0.8087248322147651, 0.8087248322147651, 0.8120805369127517, 0.8120805369127517, 0.8154362416107382]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Avg. Loss: 0.6077\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603, 9.366998192031954, 9.527456459003078, 9.685367675387365]\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[113  29]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       142\n",
      "         1.0       0.82      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[113  29]\n",
      " [ 25 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       142\n",
      "         1.0       0.82      0.84      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859, 0.7919463087248322, 0.7953020134228188, 0.802013422818792, 0.802013422818792, 0.8087248322147651, 0.8087248322147651, 0.8087248322147651, 0.8087248322147651, 0.8120805369127517, 0.8120805369127517, 0.8154362416107382, 0.8187919463087249]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Avg. Loss: 0.5938\n",
      "epsilon list is  [4.673786299411615, 5.160989768151488, 5.539484537225419, 5.865271576208738, 6.157872016001692, 6.427043883858531, 6.678541299191145, 6.916067897661618, 7.1422206570893225, 7.358856749638247, 7.5673669825587115, 7.768850994089822, 7.964145596349406, 8.153955972791838, 8.3388431286047, 8.519289590609267, 8.695688885267156, 8.868386263711521, 9.03766906023444, 9.203795813278603, 9.366998192031954, 9.527456459003078, 9.685367675387365, 9.840875662342482]\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[114  28]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.80      0.81       142\n",
      "         1.0       0.82      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Test Accuracy: 0.8188\n",
      "Confusion Matrix:\n",
      "[[114  28]\n",
      " [ 26 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.80      0.81       142\n",
      "         1.0       0.82      0.83      0.83       156\n",
      "\n",
      "    accuracy                           0.82       298\n",
      "   macro avg       0.82      0.82      0.82       298\n",
      "weighted avg       0.82      0.82      0.82       298\n",
      "\n",
      "Accuracy list is  [0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.47651006711409394, 0.4798657718120805, 0.5771812080536913, 0.7348993288590604, 0.7785234899328859, 0.7919463087248322, 0.7953020134228188, 0.802013422818792, 0.802013422818792, 0.8087248322147651, 0.8087248322147651, 0.8087248322147651, 0.8087248322147651, 0.8120805369127517, 0.8120805369127517, 0.8154362416107382, 0.8187919463087249, 0.8187919463087249]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from opacus import PrivacyEngine\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class HeartDiseaseModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HeartDiseaseModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "input_size = 30  # number of features\n",
    "\n",
    "\n",
    "\n",
    "cleveland = pd.read_csv('C:/Users/siddh/standardized_data.csv')\n",
    "print('Shape of DataFrame: {}'.format(cleveland.shape))\n",
    "print(cleveland.loc[1])\n",
    "\n",
    "cleveland.head()\n",
    "\n",
    "data = cleveland[~cleveland.isin(['?'])]\n",
    "data.loc[280:]\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "\n",
    "# dealing with categorical variables for better inference with the model\n",
    "def convert_encoding(data):\n",
    "    dummies=pd.get_dummies(data['sex'],prefix='sex')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('sex',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['slope'],prefix='slope')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('slope',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['exang'],prefix='exang')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('exang',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['restecg'],prefix='restecg')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('restecg',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['cp'],prefix='cp')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('cp',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['fbs'],prefix='fbs')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('fbs',axis=1,inplace=True)\n",
    "    \n",
    "    dummies=pd.get_dummies(data['ca'],prefix='ca')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('ca',axis=1,inplace=True)\n",
    "    \n",
    "    dummies=pd.get_dummies(data['thal'],prefix='thal')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('thal',axis=1,inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data=convert_encoding(data)\n",
    "\n",
    "y = data['num']\n",
    "X = data.drop(['num'], axis=1)\n",
    "y = y.to_numpy()\n",
    "X = X.to_numpy()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    return torch.from_numpy(df).float()\n",
    "\n",
    "X_traint = df_to_tensor(X_train)\n",
    "y_traint = df_to_tensor(y_train)\n",
    "X_testt = df_to_tensor(X_test)\n",
    "y_testt = df_to_tensor(y_test)\n",
    "\n",
    "train_ds = TensorDataset(X_traint, y_traint)\n",
    "test_ds = TensorDataset(X_testt, y_testt)\n",
    "\n",
    "# create data loaders\n",
    "batch_size = 5\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "delta=1e-7\n",
    "\n",
    "def train(model, train_dataloader, optimizer, epoch, epsilon_values):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        target = target.unsqueeze(1).float()\n",
    "        target = target.repeat(1, output.shape[1])\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    epsilon = float(privacy_engine.get_epsilon(delta))\n",
    "    epsilon_values.append(epsilon)\n",
    "    print('Epoch: {}, Avg. Loss: {:.4f}'.format(epoch, np.mean(losses)))\n",
    "    return epsilon_values\n",
    "\n",
    "def test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            output = model(data)\n",
    "            predicted = torch.round(output)\n",
    "            predictions.extend(predicted.tolist())\n",
    "            targets.extend(target.tolist())\n",
    "    acc = accuracy_score(targets, predictions)\n",
    "    print('Test Accuracy: {:.4f}'.format(acc))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(targets, predictions))\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(targets, predictions))\n",
    "    return acc\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "x=[\"adam\",\"SGD\"]\n",
    "for i,j in enumerate(x):\n",
    "    accuracies = []\n",
    "    epsilon_list=[]\n",
    "    epsilon_values = []\n",
    "    epsilon_list_total=[]\n",
    "    model1 = HeartDiseaseModel(input_size)\n",
    "    model2 = HeartDiseaseModel(input_size)\n",
    "    if(i==0):\n",
    "        model=model1\n",
    "    elif (i==1):\n",
    "        model=model2\n",
    "    privacy_engine = PrivacyEngine()\n",
    "    loss_fn = nn.BCELoss() # Binary Cross Entropy\n",
    "    if(j==\"adam\"):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    model, optimizer, dataloader = privacy_engine.make_private_with_epsilon(module=model,optimizer=optimizer,data_loader=train_dataloader,\n",
    "                                                         max_grad_norm=1.0,target_epsilon=10, epochs=25, target_delta= 1e-7)\n",
    "    criterion = nn.BCELoss()\n",
    "    for epoch in range(1,25):\n",
    "        epsilon_list=train(model, train_dataloader, optimizer, epoch,epsilon_values)\n",
    "        print('epsilon list is ', epsilon_list)\n",
    "        test(model, test_dataloader)\n",
    "        acc = test(model, test_dataloader)\n",
    "        accuracies.append(acc)\n",
    "        print('Accuracy list is ',accuracies)\n",
    "    \n",
    "    if (i==0):\n",
    "        accuracies=[i*100 for i in accuracies]\n",
    "        plt.plot(epsilon_list, accuracies,color=\"red\",label='ADAM optimizer')\n",
    "    elif(i==1):\n",
    "        accuracies=[i*100 for i in accuracies]\n",
    "        plt.plot(epsilon_list, accuracies,color=\"green\", label='SGD optimizer',linestyle='dashed')  \n",
    "    del model\n",
    "\n",
    "plt.xticks(range(0,10)) \n",
    "plt.yticks(range(int(accuracies[0]),100,3)) \n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Change in Accuracy with Epsilon')\n",
    "plt.legend(loc ='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
