{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f17511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: (1190, 12)\n",
      "age                     49.0\n",
      "sex                      0.0\n",
      "chest pain type          3.0\n",
      "resting bp s           160.0\n",
      "cholesterol            180.0\n",
      "fasting blood sugar      0.0\n",
      "resting ecg              0.0\n",
      "max heart rate         156.0\n",
      "exercise angina          0.0\n",
      "oldpeak                  1.0\n",
      "ST slope                 2.0\n",
      "target                   1.0\n",
      "Name: 1, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg. Loss: 0.6940, Epsilon:3.224408\n",
      "Epoch: 2, Avg. Loss: 0.6904, Epsilon:3.547584\n",
      "Epoch: 3, Avg. Loss: 0.6843, Epsilon:3.791562\n",
      "Epoch: 4, Avg. Loss: 0.6745, Epsilon:4.001491\n",
      "Epoch: 5, Avg. Loss: 0.6520, Epsilon:4.191486\n",
      "Epoch: 6, Avg. Loss: 0.6073, Epsilon:4.367924\n",
      "Epoch: 7, Avg. Loss: 0.5288, Epsilon:4.534301\n",
      "Epoch: 8, Avg. Loss: 0.4475, Epsilon:4.692778\n",
      "Epoch: 9, Avg. Loss: 0.4324, Epsilon:4.844799\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 21 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       107\n",
      "         1.0       0.85      0.84      0.84       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from opacus import PrivacyEngine\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb = SummaryWriter()\n",
    "\n",
    "class HeartDiseaseModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HeartDiseaseModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "input_size = 22  # number of features\n",
    "\n",
    "model = HeartDiseaseModel(input_size)\n",
    "\n",
    "import pandas as pd\n",
    "cleveland = pd.read_csv('C:/Users/siddh/heart_statlog_cleveland_hungary_final.csv')\n",
    "print('Shape of DataFrame: {}'.format(cleveland.shape))\n",
    "print(cleveland.loc[1])\n",
    "\n",
    "cleveland.head()\n",
    "\n",
    "data = cleveland[~cleveland.isin(['?'])]\n",
    "data.loc[280:]\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "#renaming columns\n",
    "data=data.rename(columns={'chest pain type':'cps','resting bp s':'rbps','fasting blood sugar':'fbs','resting ecg':'recg','max heart rate':'max_heart_rate','exercise angina':'ex_angina','ST slope':'STslope'})\n",
    "\n",
    "#dealing with categorical variables for better inference with the model\n",
    "\n",
    "def convert_encoding(data):\n",
    "    dummies=pd.get_dummies(data['sex'],prefix='sex')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('sex',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['STslope'],prefix='STslope')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('STslope',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['ex_angina'],prefix='ex_angina')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('ex_angina',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['recg'],prefix='recg')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('recg',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['cps'],prefix='cps')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('cps',axis=1,inplace=True)\n",
    "\n",
    "    dummies=pd.get_dummies(data['fbs'],prefix='fbs')\n",
    "    data=pd.concat([data,dummies],axis=1)\n",
    "    data.drop('fbs',axis=1,inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data=convert_encoding(data)\n",
    "\n",
    "y = data['target']\n",
    "X = data.drop(['target'], axis=1)\n",
    "y = y.to_numpy()\n",
    "X = X.to_numpy()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    return torch.from_numpy(df).float()\n",
    "\n",
    "X_traint = df_to_tensor(X_train)\n",
    "y_traint = df_to_tensor(y_train)\n",
    "X_testt = df_to_tensor(X_test)\n",
    "y_testt = df_to_tensor(y_test)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(X_traint, y_traint)\n",
    "test_ds = TensorDataset(X_testt, y_testt)\n",
    "\n",
    "# create data loaders\n",
    "batch_size = 5\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "delta=1e-7\n",
    "\n",
    "privacy_engine = PrivacyEngine()\n",
    "loss_fn = nn.BCELoss() # Binary Cross Entropy\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "model, optimizer, dataloader = privacy_engine.make_private_with_epsilon(module=model,optimizer=optimizer,data_loader=train_dataloader,\n",
    "                                                         max_grad_norm=1.0,target_epsilon=5, epochs=10, target_delta= 1e-7)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "def train(model, train_dataloader, optimizer, epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    total_loss = 0\n",
    "    for batch, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        target = target.unsqueeze(1).float() \n",
    "        target = target.repeat(1, output.shape[1])\n",
    "        loss = criterion(output, target)\n",
    "        total_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        epsilon= float(privacy_engine.get_epsilon(delta))\n",
    "    tb.add_scalar(\"Loss\", total_loss, epoch)\n",
    "#     tb.add_scalar(\"Correct\", total_correct, epoch)\n",
    "#     tb.add_scalar(\"Accuracy\", total_correct/ len(train_set), epoch)\n",
    "    print('Epoch: {}, Avg. Loss: {:.4f}, Epsilon:{:.6f}'.format(epoch, np.mean(losses), epsilon))\n",
    "\n",
    "def test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            output = model(data)\n",
    "            predicted = torch.round(output)\n",
    "            predictions.extend(predicted.tolist())\n",
    "            targets.extend(target.tolist())\n",
    "    acc = accuracy_score(targets, predictions)\n",
    "    print('Test Accuracy: {:.4f}'.format(acc))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(targets, predictions))\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(targets, predictions))\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(model, train_dataloader, optimizer, epoch)\n",
    "    \n",
    "test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd49385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
