{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4a7b878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: (1190, 12)\n",
      "age                     49.0\n",
      "sex                      0.0\n",
      "chest pain type          3.0\n",
      "resting bp s           160.0\n",
      "cholesterol            180.0\n",
      "fasting blood sugar      0.0\n",
      "resting ecg              0.0\n",
      "max heart rate         156.0\n",
      "exercise angina          0.0\n",
      "oldpeak                  1.0\n",
      "ST slope                 2.0\n",
      "target                   1.0\n",
      "Name: 1, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg. Loss: 0.6903\n",
      "epsilon list is  [3.4955727715186646]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Avg. Loss: 0.6859\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg. Loss: 0.6695\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733]\n",
      "Test Accuracy: 0.7017\n",
      "Confusion Matrix:\n",
      "[[ 40  67]\n",
      " [  4 127]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.37      0.53       107\n",
      "         1.0       0.65      0.97      0.78       131\n",
      "\n",
      "    accuracy                           0.70       238\n",
      "   macro avg       0.78      0.67      0.66       238\n",
      "weighted avg       0.77      0.70      0.67       238\n",
      "\n",
      "Test Accuracy: 0.7017\n",
      "Confusion Matrix:\n",
      "[[ 40  67]\n",
      " [  4 127]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.37      0.53       107\n",
      "         1.0       0.65      0.97      0.78       131\n",
      "\n",
      "    accuracy                           0.70       238\n",
      "   macro avg       0.78      0.67      0.66       238\n",
      "weighted avg       0.77      0.70      0.67       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Avg. Loss: 0.6447\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059]\n",
      "Test Accuracy: 0.7815\n",
      "Confusion Matrix:\n",
      "[[ 63  44]\n",
      " [  8 123]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.59      0.71       107\n",
      "         1.0       0.74      0.94      0.83       131\n",
      "\n",
      "    accuracy                           0.78       238\n",
      "   macro avg       0.81      0.76      0.77       238\n",
      "weighted avg       0.80      0.78      0.77       238\n",
      "\n",
      "Test Accuracy: 0.7815\n",
      "Confusion Matrix:\n",
      "[[ 63  44]\n",
      " [  8 123]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.59      0.71       107\n",
      "         1.0       0.74      0.94      0.83       131\n",
      "\n",
      "    accuracy                           0.78       238\n",
      "   macro avg       0.81      0.76      0.77       238\n",
      "weighted avg       0.80      0.78      0.77       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Avg. Loss: 0.6016\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188]\n",
      "Test Accuracy: 0.7941\n",
      "Confusion Matrix:\n",
      "[[ 71  36]\n",
      " [ 13 118]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.66      0.74       107\n",
      "         1.0       0.77      0.90      0.83       131\n",
      "\n",
      "    accuracy                           0.79       238\n",
      "   macro avg       0.81      0.78      0.79       238\n",
      "weighted avg       0.80      0.79      0.79       238\n",
      "\n",
      "Test Accuracy: 0.7941\n",
      "Confusion Matrix:\n",
      "[[ 71  36]\n",
      " [ 13 118]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.66      0.74       107\n",
      "         1.0       0.77      0.90      0.83       131\n",
      "\n",
      "    accuracy                           0.79       238\n",
      "   macro avg       0.81      0.78      0.79       238\n",
      "weighted avg       0.80      0.79      0.79       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Avg. Loss: 0.5381\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Avg. Loss: 0.4527\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Avg. Loss: 0.4039\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086]\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.76      0.78       107\n",
      "         1.0       0.81      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.76      0.78       107\n",
      "         1.0       0.81      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Avg. Loss: 0.4132\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.77      0.79       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.77      0.79       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Avg. Loss: 0.4411\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461]\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.76      0.78       107\n",
      "         1.0       0.81      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.76      0.78       107\n",
      "         1.0       0.81      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Avg. Loss: 0.4616\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488]\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.76      0.79       107\n",
      "         1.0       0.81      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.81       238\n",
      "weighted avg       0.82      0.82      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.76      0.79       107\n",
      "         1.0       0.81      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.81       238\n",
      "weighted avg       0.82      0.82      0.81       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Avg. Loss: 0.5080\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.77      0.79       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.77      0.79       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Avg. Loss: 0.5206\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.77      0.79       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.77      0.79       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Avg. Loss: 0.5128\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.78      0.80       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.78      0.80       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Avg. Loss: 0.5772\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.78      0.80       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.78      0.80       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Avg. Loss: 0.5796\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.78      0.80       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.78      0.80       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Avg. Loss: 0.5573\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.78      0.80       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.78      0.80       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Avg. Loss: 0.5787\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.77      0.79       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.77      0.79       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Avg. Loss: 0.5591\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.77      0.79       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.77      0.79       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Avg. Loss: 0.6040\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Avg. Loss: 0.6580\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441]\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 21 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.78      0.79       107\n",
      "         1.0       0.82      0.84      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 21 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.78      0.79       107\n",
      "         1.0       0.82      0.84      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Avg. Loss: 0.5947\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658]\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 21 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.78      0.79       107\n",
      "         1.0       0.82      0.84      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 21 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.78      0.79       107\n",
      "         1.0       0.82      0.84      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Avg. Loss: 0.6236\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204]\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.82      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.82      0.81       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Avg. Loss: 0.5906\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.78      0.80       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.78      0.80       107\n",
      "         1.0       0.82      0.86      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Avg. Loss: 0.5960\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Avg. Loss: 0.5997\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Avg. Loss: 0.6496\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617]\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.82      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.82      0.81       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Avg. Loss: 0.6440\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Avg. Loss: 0.6565\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.79      0.80       107\n",
      "         1.0       0.83      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.79      0.80       107\n",
      "         1.0       0.83      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Avg. Loss: 0.6997\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Avg. Loss: 0.6741\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Avg. Loss: 0.6589\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262]\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.82      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.82      0.81       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Avg. Loss: 0.6747\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.79      0.80       107\n",
      "         1.0       0.83      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.79      0.80       107\n",
      "         1.0       0.83      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Avg. Loss: 0.6971\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Avg. Loss: 0.6897\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885]\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.76      0.78       107\n",
      "         1.0       0.81      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.76      0.78       107\n",
      "         1.0       0.81      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Avg. Loss: 0.6468\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       107\n",
      "         1.0       0.83      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       107\n",
      "         1.0       0.83      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Avg. Loss: 0.6718\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188]\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.77      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.82      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.77      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.82      0.81       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058, 0.8151260504201681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Avg. Loss: 0.7091\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058, 0.8151260504201681, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Avg. Loss: 0.7251\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058, 0.8151260504201681, 0.8277310924369747, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Avg. Loss: 0.7358\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       107\n",
      "         1.0       0.83      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       107\n",
      "         1.0       0.83      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058, 0.8151260504201681, 0.8277310924369747, 0.819327731092437, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Avg. Loss: 0.7612\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058, 0.8151260504201681, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Avg. Loss: 0.6432\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 21 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.80      0.80       107\n",
      "         1.0       0.84      0.84      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 21 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.80      0.80       107\n",
      "         1.0       0.84      0.84      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058, 0.8151260504201681, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8277310924369747, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Avg. Loss: 0.7345\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058, 0.8151260504201681, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8277310924369747, 0.8235294117647058, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Avg. Loss: 0.7039\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062, 9.427278682120717]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058, 0.8151260504201681, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8277310924369747, 0.8235294117647058, 0.8277310924369747, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Avg. Loss: 0.6704\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062, 9.427278682120717, 9.523378258426778]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058, 0.8151260504201681, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8277310924369747, 0.8235294117647058, 0.8277310924369747, 0.8277310924369747, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Avg. Loss: 0.6331\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062, 9.427278682120717, 9.523378258426778, 9.618756854313268]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81       107\n",
      "         1.0       0.84      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81       107\n",
      "         1.0       0.84      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058, 0.8151260504201681, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8277310924369747, 0.8235294117647058, 0.8277310924369747, 0.8277310924369747, 0.8277310924369747, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Avg. Loss: 0.6450\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062, 9.427278682120717, 9.523378258426778, 9.618756854313268, 9.713432155511468]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81       107\n",
      "         1.0       0.83      0.87      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81       107\n",
      "         1.0       0.83      0.87      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058, 0.8151260504201681, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8277310924369747, 0.8235294117647058, 0.8277310924369747, 0.8277310924369747, 0.8277310924369747, 0.8319327731092437, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Avg. Loss: 0.6265\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062, 9.427278682120717, 9.523378258426778, 9.618756854313268, 9.713432155511468, 9.807442656469354]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.78      0.81       107\n",
      "         1.0       0.83      0.88      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.78      0.81       107\n",
      "         1.0       0.83      0.88      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058, 0.8151260504201681, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8277310924369747, 0.8235294117647058, 0.8277310924369747, 0.8277310924369747, 0.8277310924369747, 0.8319327731092437, 0.8319327731092437, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Avg. Loss: 0.6615\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062, 9.427278682120717, 9.523378258426778, 9.618756854313268, 9.713432155511468, 9.807442656469354, 9.900784164965359]\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.77      0.78       107\n",
      "         1.0       0.82      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.77      0.78       107\n",
      "         1.0       0.82      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.7016806722689075, 0.7815126050420168, 0.7941176470588235, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.819327731092437, 0.8109243697478992, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8109243697478992, 0.8151260504201681, 0.8235294117647058, 0.819327731092437, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8277310924369747, 0.819327731092437, 0.8151260504201681, 0.819327731092437, 0.819327731092437, 0.8109243697478992, 0.8235294117647058, 0.8151260504201681, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8277310924369747, 0.8235294117647058, 0.8277310924369747, 0.8277310924369747, 0.8277310924369747, 0.8319327731092437, 0.8319327731092437, 0.8319327731092437, 0.8109243697478992]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from opacus import PrivacyEngine\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "##Making the model \n",
    "class HeartDiseaseModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HeartDiseaseModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "## Added 5 layers with a dropout layer\n",
    "input_size = 22  # number of features\n",
    "\n",
    "model = HeartDiseaseModel(input_size)\n",
    "\n",
    "cleveland = pd.read_csv('C:/Users/siddh/heart_statlog_cleveland_hungary_final.csv')\n",
    "print('Shape of DataFrame: {}'.format(cleveland.shape))\n",
    "print(cleveland.loc[1])\n",
    "\n",
    "cleveland.head()\n",
    "\n",
    "data = cleveland[~cleveland.isin(['?'])]\n",
    "data.loc[280:]\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "# renaming columns\n",
    "data = data.rename(columns={'chest pain type': 'cps', 'resting bp s': 'rbps', 'fasting blood sugar': 'fbs',\n",
    "                            'resting ecg': 'recg', 'max heart rate': 'max_heart_rate', 'exercise angina': 'ex_angina',\n",
    "                            'ST slope': 'STslope'})\n",
    "\n",
    "# dealing with categorical variables for better inference with the model\n",
    "def convert_encoding(data):\n",
    "    dummies = pd.get_dummies(data['sex'], prefix='sex')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('sex', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['STslope'], prefix='STslope')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('STslope', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['ex_angina'], prefix='ex_angina')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('ex_angina', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['recg'], prefix='recg')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('recg', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['cps'], prefix='cps')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('cps', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['fbs'], prefix='fbs')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('fbs', axis=1, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = convert_encoding(data)\n",
    "## Splitting data to fit into our model with standardization\n",
    "y = data['target']\n",
    "X = data.drop(['target'], axis=1)\n",
    "y = y.to_numpy()\n",
    "X = X.to_numpy()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    return torch.from_numpy(df).float()\n",
    "\n",
    "X_traint = df_to_tensor(X_train)\n",
    "y_traint = df_to_tensor(y_train)\n",
    "X_testt = df_to_tensor(X_test)\n",
    "y_testt = df_to_tensor(y_test)\n",
    "\n",
    "train_ds = TensorDataset(X_traint, y_traint)\n",
    "test_ds = TensorDataset(X_testt, y_testt)\n",
    "\n",
    "# create data loaders\n",
    "batch_size = 5\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "delta=1e-7\n",
    "## Privacy engine object is used to call the opacus privacy engine class\n",
    "privacy_engine = PrivacyEngine()\n",
    "loss_fn = nn.BCELoss() # Binary Cross Entropy for binary losses\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "model, optimizer, dataloader = privacy_engine.make_private_with_epsilon(module=model,optimizer=optimizer,data_loader=train_dataloader,\n",
    "                                                         max_grad_norm=1.0,target_epsilon=10, epochs=50, target_delta= 1e-7)\n",
    "criterion = nn.BCELoss()\n",
    "epsilon_values = []\n",
    "def train(model, train_dataloader, optimizer, epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        target = target.unsqueeze(1).float() ##reorganizing the output to compare to the output from the file\n",
    "        target = target.repeat(1, output.shape[1])\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    epsilon = float(privacy_engine.get_epsilon(delta))\n",
    "    epsilon_values.append(epsilon)\n",
    "    print('Epoch: {}, Avg. Loss: {:.4f}'.format(epoch, np.mean(losses)))\n",
    "    return epsilon_values\n",
    "\n",
    "def test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            output = model(data)\n",
    "            predicted = torch.round(output)\n",
    "            predictions.extend(predicted.tolist()) ##converting the output to a list for accuracy check with the output split of the dataset\n",
    "            targets.extend(target.tolist())\n",
    "    acc = accuracy_score(targets, predictions)\n",
    "    print('Test Accuracy: {:.4f}'.format(acc))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(targets, predictions))\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(targets, predictions))\n",
    "    return acc\n",
    "\n",
    "\n",
    "accuracies = []\n",
    "epsilon_list=[]\n",
    "epsilon_list_total=[]\n",
    "for epoch in range(1, 50):\n",
    "    epsilon_list=train(model, train_dataloader, optimizer, epoch)\n",
    "    print('epsilon list is ', epsilon_list)\n",
    "    test(model, test_dataloader)\n",
    "    \n",
    "    acc = test(model, test_dataloader)\n",
    "    accuracies.append(acc)\n",
    "    print('Accuracy list is ',accuracies)\n",
    "\n",
    "plt.plot(epsilon_list, accuracies)\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Change in Accuracy with Epsilon')\n",
    "# for x,y in zip(epsilon_list,accuracies):\n",
    "#     count = 0\n",
    "#     label = 'epoch'+ str(count)\n",
    "#     if(count%4==0):\n",
    "#         plt.annotate(label,(x,y),textcoords='offset points',xytext=(0,10),ha='center')\n",
    "#     else:\n",
    "#         continue\n",
    "#     count+=1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a8fa215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: (1190, 12)\n",
      "age                     49.0\n",
      "sex                      0.0\n",
      "chest pain type          3.0\n",
      "resting bp s           160.0\n",
      "cholesterol            180.0\n",
      "fasting blood sugar      0.0\n",
      "resting ecg              0.0\n",
      "max heart rate         156.0\n",
      "exercise angina          0.0\n",
      "oldpeak                  1.0\n",
      "ST slope                 2.0\n",
      "target                   1.0\n",
      "Name: 1, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg. Loss: 0.6925\n",
      "epsilon list is  [5.965658927193252]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Avg. Loss: 0.6893\n",
      "epsilon list is  [5.965658927193252, 6.713814301797303]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg. Loss: 0.6811\n",
      "epsilon list is  [5.965658927193252, 6.713814301797303, 7.291278977030801]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Avg. Loss: 0.6578\n",
      "epsilon list is  [5.965658927193252, 6.713814301797303, 7.291278977030801, 7.782704112833069]\n",
      "Test Accuracy: 0.5714\n",
      "Confusion Matrix:\n",
      "[[  5 102]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.05      0.09       107\n",
      "         1.0       0.56      1.00      0.72       131\n",
      "\n",
      "    accuracy                           0.57       238\n",
      "   macro avg       0.78      0.52      0.40       238\n",
      "weighted avg       0.76      0.57      0.44       238\n",
      "\n",
      "Test Accuracy: 0.5714\n",
      "Confusion Matrix:\n",
      "[[  5 102]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.05      0.09       107\n",
      "         1.0       0.56      1.00      0.72       131\n",
      "\n",
      "    accuracy                           0.57       238\n",
      "   macro avg       0.78      0.52      0.40       238\n",
      "weighted avg       0.76      0.57      0.44       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5714285714285714]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Avg. Loss: 0.6162\n",
      "epsilon list is  [5.965658927193252, 6.713814301797303, 7.291278977030801, 7.782704112833069, 8.220710527012692]\n",
      "Test Accuracy: 0.7059\n",
      "Confusion Matrix:\n",
      "[[ 41  66]\n",
      " [  4 127]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.38      0.54       107\n",
      "         1.0       0.66      0.97      0.78       131\n",
      "\n",
      "    accuracy                           0.71       238\n",
      "   macro avg       0.78      0.68      0.66       238\n",
      "weighted avg       0.77      0.71      0.67       238\n",
      "\n",
      "Test Accuracy: 0.7059\n",
      "Confusion Matrix:\n",
      "[[ 41  66]\n",
      " [  4 127]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.38      0.54       107\n",
      "         1.0       0.66      0.97      0.78       131\n",
      "\n",
      "    accuracy                           0.71       238\n",
      "   macro avg       0.78      0.68      0.66       238\n",
      "weighted avg       0.77      0.71      0.67       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5714285714285714, 0.7058823529411765]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Avg. Loss: 0.5538\n",
      "epsilon list is  [5.965658927193252, 6.713814301797303, 7.291278977030801, 7.782704112833069, 8.220710527012692, 8.621576638865651]\n",
      "Test Accuracy: 0.7521\n",
      "Confusion Matrix:\n",
      "[[ 54  53]\n",
      " [  6 125]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.50      0.65       107\n",
      "         1.0       0.70      0.95      0.81       131\n",
      "\n",
      "    accuracy                           0.75       238\n",
      "   macro avg       0.80      0.73      0.73       238\n",
      "weighted avg       0.79      0.75      0.74       238\n",
      "\n",
      "Test Accuracy: 0.7521\n",
      "Confusion Matrix:\n",
      "[[ 54  53]\n",
      " [  6 125]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.50      0.65       107\n",
      "         1.0       0.70      0.95      0.81       131\n",
      "\n",
      "    accuracy                           0.75       238\n",
      "   macro avg       0.80      0.73      0.73       238\n",
      "weighted avg       0.79      0.75      0.74       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5714285714285714, 0.7058823529411765, 0.7521008403361344]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Avg. Loss: 0.5057\n",
      "epsilon list is  [5.965658927193252, 6.713814301797303, 7.291278977030801, 7.782704112833069, 8.220710527012692, 8.621576638865651, 8.994734495796676]\n",
      "Test Accuracy: 0.8025\n",
      "Confusion Matrix:\n",
      "[[ 70  37]\n",
      " [ 10 121]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.65      0.75       107\n",
      "         1.0       0.77      0.92      0.84       131\n",
      "\n",
      "    accuracy                           0.80       238\n",
      "   macro avg       0.82      0.79      0.79       238\n",
      "weighted avg       0.81      0.80      0.80       238\n",
      "\n",
      "Test Accuracy: 0.8025\n",
      "Confusion Matrix:\n",
      "[[ 70  37]\n",
      " [ 10 121]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.65      0.75       107\n",
      "         1.0       0.77      0.92      0.84       131\n",
      "\n",
      "    accuracy                           0.80       238\n",
      "   macro avg       0.82      0.79      0.79       238\n",
      "weighted avg       0.81      0.80      0.80       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5714285714285714, 0.7058823529411765, 0.7521008403361344, 0.8025210084033614]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Avg. Loss: 0.4494\n",
      "epsilon list is  [5.965658927193252, 6.713814301797303, 7.291278977030801, 7.782704112833069, 8.220710527012692, 8.621576638865651, 8.994734495796676, 9.346186568951403]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 80  27]\n",
      " [ 14 117]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.75      0.80       107\n",
      "         1.0       0.81      0.89      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 80  27]\n",
      " [ 14 117]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.75      0.80       107\n",
      "         1.0       0.81      0.89      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5714285714285714, 0.7058823529411765, 0.7521008403361344, 0.8025210084033614, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Avg. Loss: 0.4334\n",
      "epsilon list is  [5.965658927193252, 6.713814301797303, 7.291278977030801, 7.782704112833069, 8.220710527012692, 8.621576638865651, 8.994734495796676, 9.346186568951403, 9.680001935816076]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.76      0.79       107\n",
      "         1.0       0.82      0.88      0.85       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.76      0.79       107\n",
      "         1.0       0.82      0.88      0.85       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5714285714285714, 0.7058823529411765, 0.7521008403361344, 0.8025210084033614, 0.8277310924369747, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg. Loss: 0.6881\n",
      "epsilon list is  [4.548992081714071]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Avg. Loss: 0.6797\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg. Loss: 0.6611\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448]\n",
      "Test Accuracy: 0.6681\n",
      "Confusion Matrix:\n",
      "[[ 29  78]\n",
      " [  1 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.27      0.42       107\n",
      "         1.0       0.62      0.99      0.77       131\n",
      "\n",
      "    accuracy                           0.67       238\n",
      "   macro avg       0.80      0.63      0.60       238\n",
      "weighted avg       0.78      0.67      0.61       238\n",
      "\n",
      "Test Accuracy: 0.6681\n",
      "Confusion Matrix:\n",
      "[[ 29  78]\n",
      " [  1 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.27      0.42       107\n",
      "         1.0       0.62      0.99      0.77       131\n",
      "\n",
      "    accuracy                           0.67       238\n",
      "   macro avg       0.80      0.63      0.60       238\n",
      "weighted avg       0.78      0.67      0.61       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Avg. Loss: 0.6224\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641]\n",
      "Test Accuracy: 0.7689\n",
      "Confusion Matrix:\n",
      "[[ 58  49]\n",
      " [  6 125]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.54      0.68       107\n",
      "         1.0       0.72      0.95      0.82       131\n",
      "\n",
      "    accuracy                           0.77       238\n",
      "   macro avg       0.81      0.75      0.75       238\n",
      "weighted avg       0.80      0.77      0.76       238\n",
      "\n",
      "Test Accuracy: 0.7689\n",
      "Confusion Matrix:\n",
      "[[ 58  49]\n",
      " [  6 125]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.54      0.68       107\n",
      "         1.0       0.72      0.95      0.82       131\n",
      "\n",
      "    accuracy                           0.77       238\n",
      "   macro avg       0.81      0.75      0.75       238\n",
      "weighted avg       0.80      0.77      0.76       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Avg. Loss: 0.5578\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348]\n",
      "Test Accuracy: 0.8067\n",
      "Confusion Matrix:\n",
      "[[ 72  35]\n",
      " [ 11 120]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.67      0.76       107\n",
      "         1.0       0.77      0.92      0.84       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.82      0.79      0.80       238\n",
      "weighted avg       0.82      0.81      0.80       238\n",
      "\n",
      "Test Accuracy: 0.8067\n",
      "Confusion Matrix:\n",
      "[[ 72  35]\n",
      " [ 11 120]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.67      0.76       107\n",
      "         1.0       0.77      0.92      0.84       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.82      0.79      0.80       238\n",
      "weighted avg       0.82      0.81      0.80       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Avg. Loss: 0.4850\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975]\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 78  29]\n",
      " [ 15 116]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.73      0.78       107\n",
      "         1.0       0.80      0.89      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.81       238\n",
      "weighted avg       0.82      0.82      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 78  29]\n",
      " [ 15 116]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.73      0.78       107\n",
      "         1.0       0.80      0.89      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.81       238\n",
      "weighted avg       0.82      0.82      0.81       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Avg. Loss: 0.4209\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055]\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 15 116]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.78      0.81       107\n",
      "         1.0       0.83      0.89      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 15 116]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.78      0.81       107\n",
      "         1.0       0.83      0.89      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Avg. Loss: 0.4186\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       107\n",
      "         1.0       0.85      0.88      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       107\n",
      "         1.0       0.85      0.88      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Avg. Loss: 0.4302\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 88  19]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       107\n",
      "         1.0       0.86      0.86      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 88  19]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       107\n",
      "         1.0       0.86      0.86      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Avg. Loss: 0.4815\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 88  19]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       107\n",
      "         1.0       0.86      0.86      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 88  19]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       107\n",
      "         1.0       0.86      0.86      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Avg. Loss: 0.5551\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064]\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.80      0.82       107\n",
      "         1.0       0.84      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.80      0.82       107\n",
      "         1.0       0.84      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Avg. Loss: 0.5525\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064, 7.705160667805635]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Avg. Loss: 0.5432\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064, 7.705160667805635, 7.905566706832032]\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.82       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.82       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126, 0.8319327731092437, 0.8361344537815126]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Avg. Loss: 0.5721\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064, 7.705160667805635, 7.905566706832032, 8.100396877395495]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126, 0.8319327731092437, 0.8361344537815126, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Avg. Loss: 0.6125\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064, 7.705160667805635, 7.905566706832032, 8.100396877395495, 8.290225215263868]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126, 0.8319327731092437, 0.8361344537815126, 0.8319327731092437, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Avg. Loss: 0.5993\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064, 7.705160667805635, 7.905566706832032, 8.100396877395495, 8.290225215263868, 8.47552513205644]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 89  18]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.83      0.83       107\n",
      "         1.0       0.86      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 89  18]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.83      0.83       107\n",
      "         1.0       0.86      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126, 0.8319327731092437, 0.8361344537815126, 0.8319327731092437, 0.8319327731092437, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Avg. Loss: 0.6048\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064, 7.705160667805635, 7.905566706832032, 8.100396877395495, 8.290225215263868, 8.47552513205644, 8.656701805368058]\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.82       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.81      0.82       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126, 0.8319327731092437, 0.8361344537815126, 0.8319327731092437, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Avg. Loss: 0.6862\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064, 7.705160667805635, 7.905566706832032, 8.100396877395495, 8.290225215263868, 8.47552513205644, 8.656701805368058, 8.834105445185902]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       107\n",
      "         1.0       0.83      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       107\n",
      "         1.0       0.83      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126, 0.8319327731092437, 0.8361344537815126, 0.8319327731092437, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Avg. Loss: 0.6550\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064, 7.705160667805635, 7.905566706832032, 8.100396877395495, 8.290225215263868, 8.47552513205644, 8.656701805368058, 8.834105445185902, 9.008022160810407]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81       107\n",
      "         1.0       0.84      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81       107\n",
      "         1.0       0.84      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126, 0.8319327731092437, 0.8361344537815126, 0.8319327731092437, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8277310924369747, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Avg. Loss: 0.6565\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064, 7.705160667805635, 7.905566706832032, 8.100396877395495, 8.290225215263868, 8.47552513205644, 8.656701805368058, 8.834105445185902, 9.008022160810407, 9.178711845232396]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81       107\n",
      "         1.0       0.84      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81       107\n",
      "         1.0       0.84      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126, 0.8319327731092437, 0.8361344537815126, 0.8319327731092437, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8277310924369747, 0.8319327731092437, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Avg. Loss: 0.6617\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064, 7.705160667805635, 7.905566706832032, 8.100396877395495, 8.290225215263868, 8.47552513205644, 8.656701805368058, 8.834105445185902, 9.008022160810407, 9.178711845232396, 9.346410528981652]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126, 0.8319327731092437, 0.8361344537815126, 0.8319327731092437, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8277310924369747, 0.8319327731092437, 0.8319327731092437, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Avg. Loss: 0.6638\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064, 7.705160667805635, 7.905566706832032, 8.100396877395495, 8.290225215263868, 8.47552513205644, 8.656701805368058, 8.834105445185902, 9.008022160810407, 9.178711845232396, 9.346410528981652, 9.511310468846988]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126, 0.8319327731092437, 0.8361344537815126, 0.8319327731092437, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8277310924369747, 0.8319327731092437, 0.8319327731092437, 0.8277310924369747, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Avg. Loss: 0.6962\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064, 7.705160667805635, 7.905566706832032, 8.100396877395495, 8.290225215263868, 8.47552513205644, 8.656701805368058, 8.834105445185902, 9.008022160810407, 9.178711845232396, 9.346410528981652, 9.511310468846988, 9.673593626268275]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126, 0.8319327731092437, 0.8361344537815126, 0.8319327731092437, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8277310924369747, 0.8319327731092437, 0.8319327731092437, 0.8277310924369747, 0.8319327731092437, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Avg. Loss: 0.6399\n",
      "epsilon list is  [4.548992081714071, 5.0412861828597295, 5.42574646572448, 5.757472508626641, 6.055918621537348, 6.330833808994975, 6.5879735559733055, 6.831069091405361, 7.062684277676816, 7.284693658487299, 7.498491022548064, 7.705160667805635, 7.905566706832032, 8.100396877395495, 8.290225215263868, 8.47552513205644, 8.656701805368058, 8.834105445185902, 9.008022160810407, 9.178711845232396, 9.346410528981652, 9.511310468846988, 9.673593626268275, 9.833414753759072]\n",
      "Test Accuracy: 0.8403\n",
      "Confusion Matrix:\n",
      "[[ 88  19]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8403\n",
      "Confusion Matrix:\n",
      "[[ 88  19]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.6680672268907563, 0.7689075630252101, 0.8067226890756303, 0.8151260504201681, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8445378151260504, 0.8361344537815126, 0.8319327731092437, 0.8361344537815126, 0.8319327731092437, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8277310924369747, 0.8319327731092437, 0.8319327731092437, 0.8277310924369747, 0.8319327731092437, 0.8319327731092437, 0.8403361344537815]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg. Loss: 0.6922\n",
      "epsilon list is  [3.4955727715186646]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Avg. Loss: 0.6875\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg. Loss: 0.6805\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Avg. Loss: 0.6623\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 78  29]\n",
      " [ 12 119]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.73      0.79       107\n",
      "         1.0       0.80      0.91      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.84      0.82      0.82       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 78  29]\n",
      " [ 12 119]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.73      0.79       107\n",
      "         1.0       0.80      0.91      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.84      0.82      0.82       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Avg. Loss: 0.6152\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188]\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 92  15]\n",
      " [ 24 107]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.86      0.83       107\n",
      "         1.0       0.88      0.82      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 92  15]\n",
      " [ 24 107]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.86      0.83       107\n",
      "         1.0       0.88      0.82      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Avg. Loss: 0.5464\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682]\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 93  14]\n",
      " [ 25 106]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.87      0.83       107\n",
      "         1.0       0.88      0.81      0.84       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 93  14]\n",
      " [ 25 106]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.87      0.83       107\n",
      "         1.0       0.88      0.81      0.84       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Avg. Loss: 0.4709\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 90  17]\n",
      " [ 23 108]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.84      0.82       107\n",
      "         1.0       0.86      0.82      0.84       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 90  17]\n",
      " [ 23 108]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.84      0.82       107\n",
      "         1.0       0.86      0.82      0.84       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Avg. Loss: 0.4273\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086]\n",
      "Test Accuracy: 0.8403\n",
      "Confusion Matrix:\n",
      "[[ 90  17]\n",
      " [ 21 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.84      0.83       107\n",
      "         1.0       0.87      0.84      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8403\n",
      "Confusion Matrix:\n",
      "[[ 90  17]\n",
      " [ 21 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.84      0.83       107\n",
      "         1.0       0.87      0.84      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Avg. Loss: 0.4356\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Avg. Loss: 0.4473\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       107\n",
      "         1.0       0.83      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       107\n",
      "         1.0       0.83      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Avg. Loss: 0.4698\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488]\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8193\n",
      "Confusion Matrix:\n",
      "[[ 83  24]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.78      0.79       107\n",
      "         1.0       0.82      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Avg. Loss: 0.5254\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       107\n",
      "         1.0       0.83      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       107\n",
      "         1.0       0.83      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Avg. Loss: 0.5236\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.79      0.80       107\n",
      "         1.0       0.83      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.79      0.80       107\n",
      "         1.0       0.83      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Avg. Loss: 0.6077\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       107\n",
      "         1.0       0.83      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.80       107\n",
      "         1.0       0.83      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Avg. Loss: 0.5989\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       107\n",
      "         1.0       0.85      0.88      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       107\n",
      "         1.0       0.85      0.88      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Avg. Loss: 0.5793\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285]\n",
      "Test Accuracy: 0.8487\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 13 118]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.79      0.82       107\n",
      "         1.0       0.84      0.90      0.87       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.84      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Test Accuracy: 0.8487\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 13 118]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.79      0.82       107\n",
      "         1.0       0.84      0.90      0.87       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.84      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Avg. Loss: 0.6027\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 14 117]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.79      0.82       107\n",
      "         1.0       0.84      0.89      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.85      0.84      0.84       238\n",
      "weighted avg       0.85      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 14 117]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.79      0.82       107\n",
      "         1.0       0.84      0.89      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.85      0.84      0.84       238\n",
      "weighted avg       0.85      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Avg. Loss: 0.6636\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       107\n",
      "         1.0       0.85      0.88      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       107\n",
      "         1.0       0.85      0.88      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Avg. Loss: 0.6384\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81       107\n",
      "         1.0       0.83      0.87      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81       107\n",
      "         1.0       0.83      0.87      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Avg. Loss: 0.6353\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 12 119]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.77      0.82       107\n",
      "         1.0       0.83      0.91      0.87       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.85      0.84      0.84       238\n",
      "weighted avg       0.85      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 12 119]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.77      0.82       107\n",
      "         1.0       0.83      0.91      0.87       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.85      0.84      0.84       238\n",
      "weighted avg       0.85      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Avg. Loss: 0.6472\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441]\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.79      0.81       107\n",
      "         1.0       0.83      0.88      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 84  23]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.79      0.81       107\n",
      "         1.0       0.83      0.88      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Avg. Loss: 0.6820\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658]\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81       107\n",
      "         1.0       0.84      0.87      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 85  22]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.79      0.81       107\n",
      "         1.0       0.84      0.87      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Avg. Loss: 0.7004\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.81      0.82       107\n",
      "         1.0       0.85      0.87      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.81      0.82       107\n",
      "         1.0       0.85      0.87      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Avg. Loss: 0.6487\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.81      0.82       107\n",
      "         1.0       0.85      0.87      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.81      0.82       107\n",
      "         1.0       0.85      0.87      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Avg. Loss: 0.6794\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402]\n",
      "Test Accuracy: 0.8529\n",
      "Confusion Matrix:\n",
      "[[ 91  16]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84       107\n",
      "         1.0       0.88      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.85      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Test Accuracy: 0.8529\n",
      "Confusion Matrix:\n",
      "[[ 91  16]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84       107\n",
      "         1.0       0.88      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.85      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Avg. Loss: 0.6860\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748]\n",
      "Test Accuracy: 0.8529\n",
      "Confusion Matrix:\n",
      "[[ 90  17]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.84      0.84       107\n",
      "         1.0       0.87      0.86      0.87       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.85      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Test Accuracy: 0.8529\n",
      "Confusion Matrix:\n",
      "[[ 90  17]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.84      0.84       107\n",
      "         1.0       0.87      0.86      0.87       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.85      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Avg. Loss: 0.7085\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617]\n",
      "Test Accuracy: 0.8487\n",
      "Confusion Matrix:\n",
      "[[ 89  18]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.83      0.83       107\n",
      "         1.0       0.86      0.86      0.86       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.85      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Test Accuracy: 0.8487\n",
      "Confusion Matrix:\n",
      "[[ 89  18]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.83      0.83       107\n",
      "         1.0       0.86      0.86      0.86       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.85      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Avg. Loss: 0.6974\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132]\n",
      "Test Accuracy: 0.8403\n",
      "Confusion Matrix:\n",
      "[[ 88  19]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8403\n",
      "Confusion Matrix:\n",
      "[[ 88  19]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Avg. Loss: 0.6615\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 89  18]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.83      0.83       107\n",
      "         1.0       0.86      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 89  18]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.83      0.83       107\n",
      "         1.0       0.86      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Avg. Loss: 0.7097\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 88  19]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       107\n",
      "         1.0       0.86      0.86      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 88  19]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.82      0.83       107\n",
      "         1.0       0.86      0.86      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Avg. Loss: 0.6701\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628]\n",
      "Test Accuracy: 0.8487\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 15 116]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.80      0.83       107\n",
      "         1.0       0.85      0.89      0.87       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.84      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Test Accuracy: 0.8487\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 15 116]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.80      0.83       107\n",
      "         1.0       0.85      0.89      0.87       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.84      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Avg. Loss: 0.6445\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262]\n",
      "Test Accuracy: 0.8571\n",
      "Confusion Matrix:\n",
      "[[ 91  16]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84       107\n",
      "         1.0       0.88      0.86      0.87       131\n",
      "\n",
      "    accuracy                           0.86       238\n",
      "   macro avg       0.86      0.86      0.86       238\n",
      "weighted avg       0.86      0.86      0.86       238\n",
      "\n",
      "Test Accuracy: 0.8571\n",
      "Confusion Matrix:\n",
      "[[ 91  16]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84       107\n",
      "         1.0       0.88      0.86      0.87       131\n",
      "\n",
      "    accuracy                           0.86       238\n",
      "   macro avg       0.86      0.86      0.86       238\n",
      "weighted avg       0.86      0.86      0.86       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Avg. Loss: 0.6999\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503]\n",
      "Test Accuracy: 0.8571\n",
      "Confusion Matrix:\n",
      "[[ 91  16]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84       107\n",
      "         1.0       0.88      0.86      0.87       131\n",
      "\n",
      "    accuracy                           0.86       238\n",
      "   macro avg       0.86      0.86      0.86       238\n",
      "weighted avg       0.86      0.86      0.86       238\n",
      "\n",
      "Test Accuracy: 0.8571\n",
      "Confusion Matrix:\n",
      "[[ 91  16]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84       107\n",
      "         1.0       0.88      0.86      0.87       131\n",
      "\n",
      "    accuracy                           0.86       238\n",
      "   macro avg       0.86      0.86      0.86       238\n",
      "weighted avg       0.86      0.86      0.86       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Avg. Loss: 0.6800\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828]\n",
      "Test Accuracy: 0.8487\n",
      "Confusion Matrix:\n",
      "[[ 91  16]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.85      0.83       107\n",
      "         1.0       0.87      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.85      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Test Accuracy: 0.8487\n",
      "Confusion Matrix:\n",
      "[[ 91  16]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.85      0.83       107\n",
      "         1.0       0.87      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.85      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Avg. Loss: 0.6394\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885]\n",
      "Test Accuracy: 0.8529\n",
      "Confusion Matrix:\n",
      "[[ 91  16]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84       107\n",
      "         1.0       0.88      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.85      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Test Accuracy: 0.8529\n",
      "Confusion Matrix:\n",
      "[[ 91  16]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84       107\n",
      "         1.0       0.88      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.85      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Avg. Loss: 0.6832\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896]\n",
      "Test Accuracy: 0.8529\n",
      "Confusion Matrix:\n",
      "[[ 91  16]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84       107\n",
      "         1.0       0.88      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.85      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Test Accuracy: 0.8529\n",
      "Confusion Matrix:\n",
      "[[ 91  16]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.85      0.84       107\n",
      "         1.0       0.88      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.85       238\n",
      "   macro avg       0.85      0.85      0.85       238\n",
      "weighted avg       0.85      0.85      0.85       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Avg. Loss: 0.6752\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 90  17]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       107\n",
      "         1.0       0.87      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.85      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 90  17]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       107\n",
      "         1.0       0.87      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.85      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Avg. Loss: 0.6818\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617]\n",
      "Test Accuracy: 0.8403\n",
      "Confusion Matrix:\n",
      "[[ 90  17]\n",
      " [ 21 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.84      0.83       107\n",
      "         1.0       0.87      0.84      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8403\n",
      "Confusion Matrix:\n",
      "[[ 90  17]\n",
      " [ 21 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.84      0.83       107\n",
      "         1.0       0.87      0.84      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882, 0.8445378151260504, 0.8403361344537815]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Avg. Loss: 0.6990\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975]\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 88  19]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.82       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 88  19]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.82      0.82       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882, 0.8445378151260504, 0.8403361344537815, 0.8361344537815126]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Avg. Loss: 0.7446\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.84       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882, 0.8445378151260504, 0.8403361344537815, 0.8361344537815126, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Avg. Loss: 0.6886\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 21 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       107\n",
      "         1.0       0.85      0.84      0.84       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 21 110]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       107\n",
      "         1.0       0.85      0.84      0.84       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882, 0.8445378151260504, 0.8403361344537815, 0.8361344537815126, 0.8277310924369747, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Avg. Loss: 0.7318\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882, 0.8445378151260504, 0.8403361344537815, 0.8361344537815126, 0.8277310924369747, 0.8277310924369747, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Avg. Loss: 0.7259\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062]\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 90  17]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       107\n",
      "         1.0       0.87      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.85      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8445\n",
      "Confusion Matrix:\n",
      "[[ 90  17]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       107\n",
      "         1.0       0.87      0.85      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.85      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882, 0.8445378151260504, 0.8403361344537815, 0.8361344537815126, 0.8277310924369747, 0.8277310924369747, 0.8319327731092437, 0.8445378151260504]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Avg. Loss: 0.7530\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062, 9.427278682120717]\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.80      0.82       107\n",
      "         1.0       0.84      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8361\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.80      0.82       107\n",
      "         1.0       0.84      0.86      0.85       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.83      0.83       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882, 0.8445378151260504, 0.8403361344537815, 0.8361344537815126, 0.8277310924369747, 0.8277310924369747, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Avg. Loss: 0.8052\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062, 9.427278682120717, 9.523378258426778]\n",
      "Test Accuracy: 0.8403\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       107\n",
      "         1.0       0.85      0.86      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8403\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 18 113]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82       107\n",
      "         1.0       0.85      0.86      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882, 0.8445378151260504, 0.8403361344537815, 0.8361344537815126, 0.8277310924369747, 0.8277310924369747, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8403361344537815]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Avg. Loss: 0.7694\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062, 9.427278682120717, 9.523378258426778, 9.618756854313268]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882, 0.8445378151260504, 0.8403361344537815, 0.8361344537815126, 0.8277310924369747, 0.8277310924369747, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8403361344537815, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Avg. Loss: 0.7796\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062, 9.427278682120717, 9.523378258426778, 9.618756854313268, 9.713432155511468]\n",
      "Test Accuracy: 0.8403\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.80      0.82       107\n",
      "         1.0       0.84      0.87      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Test Accuracy: 0.8403\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.80      0.82       107\n",
      "         1.0       0.84      0.87      0.86       131\n",
      "\n",
      "    accuracy                           0.84       238\n",
      "   macro avg       0.84      0.84      0.84       238\n",
      "weighted avg       0.84      0.84      0.84       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882, 0.8445378151260504, 0.8403361344537815, 0.8361344537815126, 0.8277310924369747, 0.8277310924369747, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8403361344537815, 0.8319327731092437, 0.8403361344537815]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Avg. Loss: 0.6946\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062, 9.427278682120717, 9.523378258426778, 9.618756854313268, 9.713432155511468, 9.807442656469354]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81       107\n",
      "         1.0       0.85      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882, 0.8445378151260504, 0.8403361344537815, 0.8361344537815126, 0.8277310924369747, 0.8277310924369747, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8403361344537815, 0.8319327731092437, 0.8403361344537815, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Avg. Loss: 0.7653\n",
      "epsilon list is  [3.4955727715186646, 3.8471511002792123, 4.115189224686733, 4.346496243127059, 4.555870056610188, 4.750110677265682, 4.933010709037492, 5.106955375134086, 5.273556218121424, 5.433977254890461, 5.589084942658488, 5.739550252388477, 5.885906670146697, 6.0285828382019995, 6.167938067358773, 6.3042646961579285, 6.437818120466582, 6.568818042150299, 6.697445832964148, 6.823868943008873, 6.948229476898441, 7.070658349752658, 7.191267903700204, 7.310159629058868, 7.427429672102402, 7.54315992745748, 7.657425387238617, 7.770300307244132, 7.881841416552253, 7.992114163059741, 8.101171650761628, 8.209063608234262, 8.315834668474503, 8.421535083363828, 8.52620308650885, 8.629874766304896, 8.732585454946188, 8.834369285357617, 8.935258591950975, 9.035289436719001, 9.134473228143019, 9.232853712488879, 9.33044886295062, 9.427278682120717, 9.523378258426778, 9.618756854313268, 9.713432155511468, 9.807442656469354, 9.900784164965359]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 86  21]\n",
      " [ 19 112]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.80      0.81       107\n",
      "         1.0       0.84      0.85      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.8277310924369747, 0.8361344537815126, 0.8361344537815126, 0.8319327731092437, 0.8403361344537815, 0.8277310924369747, 0.8277310924369747, 0.819327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8445378151260504, 0.8487394957983193, 0.8445378151260504, 0.8445378151260504, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8361344537815126, 0.8445378151260504, 0.8445378151260504, 0.8529411764705882, 0.8529411764705882, 0.8487394957983193, 0.8403361344537815, 0.8445378151260504, 0.8445378151260504, 0.8487394957983193, 0.8571428571428571, 0.8571428571428571, 0.8487394957983193, 0.8529411764705882, 0.8529411764705882, 0.8445378151260504, 0.8403361344537815, 0.8361344537815126, 0.8277310924369747, 0.8277310924369747, 0.8319327731092437, 0.8445378151260504, 0.8361344537815126, 0.8403361344537815, 0.8319327731092437, 0.8403361344537815, 0.8319327731092437, 0.8319327731092437]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from opacus import PrivacyEngine\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class HeartDiseaseModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HeartDiseaseModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "input_size = 22  # number of features\n",
    "\n",
    "\n",
    "\n",
    "cleveland = pd.read_csv('C:/Users/siddh/heart_statlog_cleveland_hungary_final.csv')\n",
    "print('Shape of DataFrame: {}'.format(cleveland.shape))\n",
    "print(cleveland.loc[1])\n",
    "\n",
    "cleveland.head()\n",
    "\n",
    "data = cleveland[~cleveland.isin(['?'])]\n",
    "data.loc[280:]\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "# renaming columns\n",
    "data = data.rename(columns={'chest pain type': 'cps', 'resting bp s': 'rbps', 'fasting blood sugar': 'fbs',\n",
    "                            'resting ecg': 'recg', 'max heart rate': 'max_heart_rate', 'exercise angina': 'ex_angina',\n",
    "                            'ST slope': 'STslope'})\n",
    "\n",
    "# dealing with categorical variables for better inference with the model\n",
    "def convert_encoding(data):\n",
    "    dummies = pd.get_dummies(data['sex'], prefix='sex')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('sex', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['STslope'], prefix='STslope')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('STslope', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['ex_angina'], prefix='ex_angina')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('ex_angina', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['recg'], prefix='recg')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('recg', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['cps'], prefix='cps')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('cps', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['fbs'], prefix='fbs')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('fbs', axis=1, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = convert_encoding(data)\n",
    "\n",
    "y = data['target']\n",
    "X = data.drop(['target'], axis=1)\n",
    "y = y.to_numpy()\n",
    "X = X.to_numpy()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    return torch.from_numpy(df).float()\n",
    "\n",
    "X_traint = df_to_tensor(X_train)\n",
    "y_traint = df_to_tensor(y_train)\n",
    "X_testt = df_to_tensor(X_test)\n",
    "y_testt = df_to_tensor(y_test)\n",
    "\n",
    "train_ds = TensorDataset(X_traint, y_traint)\n",
    "test_ds = TensorDataset(X_testt, y_testt)\n",
    "\n",
    "# create data loaders\n",
    "batch_size = 5\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "delta=1e-7\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, optimizer, epoch, epsilon_values):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        target = target.unsqueeze(1).float()\n",
    "        target = target.repeat(1, output.shape[1])\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    epsilon = float(privacy_engine.get_epsilon(delta))\n",
    "    epsilon_values.append(epsilon)\n",
    "    print('Epoch: {}, Avg. Loss: {:.4f}'.format(epoch, np.mean(losses)))\n",
    "    return epsilon_values\n",
    "\n",
    "def test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            output = model(data)\n",
    "            predicted = torch.round(output)\n",
    "            predictions.extend(predicted.tolist())\n",
    "            targets.extend(target.tolist())\n",
    "    acc = accuracy_score(targets, predictions)\n",
    "    print('Test Accuracy: {:.4f}'.format(acc))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(targets, predictions))\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(targets, predictions))\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "x=[10, 25, 50]\n",
    "for i,j in enumerate(x):\n",
    "    accuracies = []\n",
    "    epsilon_list=[]\n",
    "    epsilon_values = []\n",
    "    epsilon_list_total=[]\n",
    "    model1 = HeartDiseaseModel(input_size)\n",
    "    model2 = HeartDiseaseModel(input_size)\n",
    "    model3 = HeartDiseaseModel(input_size)\n",
    "    if(i==0):\n",
    "        model=model1\n",
    "    elif (i==1):\n",
    "        model=model2\n",
    "    else:\n",
    "        model=model3\n",
    "        \n",
    "    privacy_engine = PrivacyEngine()\n",
    "    loss_fn = nn.BCELoss() # Binary Cross Entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model, optimizer, dataloader = privacy_engine.make_private_with_epsilon(module=model,optimizer=optimizer,data_loader=train_dataloader,\n",
    "                                                         max_grad_norm=1.0,target_epsilon=10, epochs=j, target_delta= 1e-7)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(1,j):\n",
    "        epsilon_list=train(model, train_dataloader, optimizer, epoch,epsilon_values)\n",
    "        print('epsilon list is ', epsilon_list)\n",
    "        test(model, test_dataloader)\n",
    "        acc = test(model, test_dataloader)\n",
    "        accuracies.append(acc)\n",
    "        print('Accuracy list is ',accuracies)\n",
    "    if (i==0):\n",
    "        accuracies=[i*100 for i in accuracies]\n",
    "        plt.plot(epsilon_list, accuracies,color=\"red\",label='10 epochs')\n",
    "    elif(i==1):\n",
    "        accuracies=[i*100 for i in accuracies]\n",
    "        plt.plot(epsilon_list, accuracies,color=\"green\", label='25 epochs')\n",
    "    else:\n",
    "        accuracies=[i*100 for i in accuracies] \n",
    "        plt.plot(epsilon_list, accuracies, color=\"blue\", label='50 epochs')    \n",
    "    del model\n",
    "\n",
    "plt.xticks(range(0,10)) \n",
    "plt.yticks(range(int(accuracies[0]),100,3)) \n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Change in Accuracy with Epsilon')\n",
    "plt.legend(loc ='upper left')\n",
    "plt.show()\n",
    "\n",
    "# for x,y in zip(epsilon_list,accuracies):\n",
    "#     count = 0\n",
    "#     label = 'epoch'+ str(count)\n",
    "#     if(count%4==0):\n",
    "#         plt.annotate(label,(x,y),textcoords='offset points',xytext=(0,10),ha='center')\n",
    "#     else:\n",
    "#         continue\n",
    "#     count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a54200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame: (1190, 12)\n",
      "age                     49.0\n",
      "sex                      0.0\n",
      "chest pain type          3.0\n",
      "resting bp s           160.0\n",
      "cholesterol            180.0\n",
      "fasting blood sugar      0.0\n",
      "resting ecg              0.0\n",
      "max heart rate         156.0\n",
      "exercise angina          0.0\n",
      "oldpeak                  1.0\n",
      "ST slope                 2.0\n",
      "target                   1.0\n",
      "Name: 1, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg. Loss: 0.6982\n",
      "epsilon list is  [0.5071015623125099]\n",
      "Test Accuracy: 0.4496\n",
      "Confusion Matrix:\n",
      "[[107   0]\n",
      " [131   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      1.00      0.62       107\n",
      "         1.0       0.00      0.00      0.00       131\n",
      "\n",
      "    accuracy                           0.45       238\n",
      "   macro avg       0.22      0.50      0.31       238\n",
      "weighted avg       0.20      0.45      0.28       238\n",
      "\n",
      "Test Accuracy: 0.4496\n",
      "Confusion Matrix:\n",
      "[[107   0]\n",
      " [131   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      1.00      0.62       107\n",
      "         1.0       0.00      0.00      0.00       131\n",
      "\n",
      "    accuracy                           0.45       238\n",
      "   macro avg       0.22      0.50      0.31       238\n",
      "weighted avg       0.20      0.45      0.28       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Avg. Loss: 0.6933\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871]\n",
      "Test Accuracy: 0.4496\n",
      "Confusion Matrix:\n",
      "[[107   0]\n",
      " [131   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      1.00      0.62       107\n",
      "         1.0       0.00      0.00      0.00       131\n",
      "\n",
      "    accuracy                           0.45       238\n",
      "   macro avg       0.22      0.50      0.31       238\n",
      "weighted avg       0.20      0.45      0.28       238\n",
      "\n",
      "Test Accuracy: 0.4496\n",
      "Confusion Matrix:\n",
      "[[107   0]\n",
      " [131   0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      1.00      0.62       107\n",
      "         1.0       0.00      0.00      0.00       131\n",
      "\n",
      "    accuracy                           0.45       238\n",
      "   macro avg       0.22      0.50      0.31       238\n",
      "weighted avg       0.20      0.45      0.28       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg. Loss: 0.6909\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592]\n",
      "Test Accuracy: 0.4496\n",
      "Confusion Matrix:\n",
      "[[105   2]\n",
      " [129   2]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.98      0.62       107\n",
      "         1.0       0.50      0.02      0.03       131\n",
      "\n",
      "    accuracy                           0.45       238\n",
      "   macro avg       0.47      0.50      0.32       238\n",
      "weighted avg       0.48      0.45      0.29       238\n",
      "\n",
      "Test Accuracy: 0.4496\n",
      "Confusion Matrix:\n",
      "[[105   2]\n",
      " [129   2]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.98      0.62       107\n",
      "         1.0       0.50      0.02      0.03       131\n",
      "\n",
      "    accuracy                           0.45       238\n",
      "   macro avg       0.47      0.50      0.32       238\n",
      "weighted avg       0.48      0.45      0.29       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Avg. Loss: 0.6870\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674]\n",
      "Test Accuracy: 0.6807\n",
      "Confusion Matrix:\n",
      "[[90 17]\n",
      " [59 72]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.84      0.70       107\n",
      "         1.0       0.81      0.55      0.65       131\n",
      "\n",
      "    accuracy                           0.68       238\n",
      "   macro avg       0.71      0.70      0.68       238\n",
      "weighted avg       0.72      0.68      0.68       238\n",
      "\n",
      "Test Accuracy: 0.6807\n",
      "Confusion Matrix:\n",
      "[[90 17]\n",
      " [59 72]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.84      0.70       107\n",
      "         1.0       0.81      0.55      0.65       131\n",
      "\n",
      "    accuracy                           0.68       238\n",
      "   macro avg       0.71      0.70      0.68       238\n",
      "weighted avg       0.72      0.68      0.68       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Avg. Loss: 0.6824\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845]\n",
      "Test Accuracy: 0.7437\n",
      "Confusion Matrix:\n",
      "[[ 58  49]\n",
      " [ 12 119]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.54      0.66       107\n",
      "         1.0       0.71      0.91      0.80       131\n",
      "\n",
      "    accuracy                           0.74       238\n",
      "   macro avg       0.77      0.73      0.73       238\n",
      "weighted avg       0.76      0.74      0.73       238\n",
      "\n",
      "Test Accuracy: 0.7437\n",
      "Confusion Matrix:\n",
      "[[ 58  49]\n",
      " [ 12 119]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.54      0.66       107\n",
      "         1.0       0.71      0.91      0.80       131\n",
      "\n",
      "    accuracy                           0.74       238\n",
      "   macro avg       0.77      0.73      0.73       238\n",
      "weighted avg       0.76      0.74      0.73       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Avg. Loss: 0.6727\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364]\n",
      "Test Accuracy: 0.6345\n",
      "Confusion Matrix:\n",
      "[[ 22  85]\n",
      " [  2 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.21      0.34       107\n",
      "         1.0       0.60      0.98      0.75       131\n",
      "\n",
      "    accuracy                           0.63       238\n",
      "   macro avg       0.76      0.60      0.54       238\n",
      "weighted avg       0.74      0.63      0.56       238\n",
      "\n",
      "Test Accuracy: 0.6345\n",
      "Confusion Matrix:\n",
      "[[ 22  85]\n",
      " [  2 129]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.21      0.34       107\n",
      "         1.0       0.60      0.98      0.75       131\n",
      "\n",
      "    accuracy                           0.63       238\n",
      "   macro avg       0.76      0.60      0.54       238\n",
      "weighted avg       0.74      0.63      0.56       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Avg. Loss: 0.6638\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342]\n",
      "Test Accuracy: 0.6639\n",
      "Confusion Matrix:\n",
      "[[ 33  74]\n",
      " [  6 125]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.31      0.45       107\n",
      "         1.0       0.63      0.95      0.76       131\n",
      "\n",
      "    accuracy                           0.66       238\n",
      "   macro avg       0.74      0.63      0.60       238\n",
      "weighted avg       0.73      0.66      0.62       238\n",
      "\n",
      "Test Accuracy: 0.6639\n",
      "Confusion Matrix:\n",
      "[[ 33  74]\n",
      " [  6 125]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.31      0.45       107\n",
      "         1.0       0.63      0.95      0.76       131\n",
      "\n",
      "    accuracy                           0.66       238\n",
      "   macro avg       0.74      0.63      0.60       238\n",
      "weighted avg       0.73      0.66      0.62       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Avg. Loss: 0.6518\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494]\n",
      "Test Accuracy: 0.7353\n",
      "Confusion Matrix:\n",
      "[[ 51  56]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.48      0.62       107\n",
      "         1.0       0.69      0.95      0.80       131\n",
      "\n",
      "    accuracy                           0.74       238\n",
      "   macro avg       0.78      0.71      0.71       238\n",
      "weighted avg       0.77      0.74      0.72       238\n",
      "\n",
      "Test Accuracy: 0.7353\n",
      "Confusion Matrix:\n",
      "[[ 51  56]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.48      0.62       107\n",
      "         1.0       0.69      0.95      0.80       131\n",
      "\n",
      "    accuracy                           0.74       238\n",
      "   macro avg       0.78      0.71      0.71       238\n",
      "weighted avg       0.77      0.74      0.72       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Avg. Loss: 0.6281\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866]\n",
      "Test Accuracy: 0.7437\n",
      "Confusion Matrix:\n",
      "[[ 53  54]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.50      0.63       107\n",
      "         1.0       0.70      0.95      0.80       131\n",
      "\n",
      "    accuracy                           0.74       238\n",
      "   macro avg       0.79      0.72      0.72       238\n",
      "weighted avg       0.78      0.74      0.73       238\n",
      "\n",
      "Test Accuracy: 0.7437\n",
      "Confusion Matrix:\n",
      "[[ 53  54]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.50      0.63       107\n",
      "         1.0       0.70      0.95      0.80       131\n",
      "\n",
      "    accuracy                           0.74       238\n",
      "   macro avg       0.79      0.72      0.72       238\n",
      "weighted avg       0.78      0.74      0.73       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Avg. Loss: 0.5916\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616]\n",
      "Test Accuracy: 0.7647\n",
      "Confusion Matrix:\n",
      "[[ 58  49]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.54      0.67       107\n",
      "         1.0       0.72      0.95      0.82       131\n",
      "\n",
      "    accuracy                           0.76       238\n",
      "   macro avg       0.80      0.74      0.75       238\n",
      "weighted avg       0.80      0.76      0.75       238\n",
      "\n",
      "Test Accuracy: 0.7647\n",
      "Confusion Matrix:\n",
      "[[ 58  49]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.54      0.67       107\n",
      "         1.0       0.72      0.95      0.82       131\n",
      "\n",
      "    accuracy                           0.76       238\n",
      "   macro avg       0.80      0.74      0.75       238\n",
      "weighted avg       0.80      0.76      0.75       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Avg. Loss: 0.5532\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294]\n",
      "Test Accuracy: 0.7563\n",
      "Confusion Matrix:\n",
      "[[ 56  51]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.52      0.66       107\n",
      "         1.0       0.71      0.95      0.81       131\n",
      "\n",
      "    accuracy                           0.76       238\n",
      "   macro avg       0.80      0.73      0.73       238\n",
      "weighted avg       0.79      0.76      0.74       238\n",
      "\n",
      "Test Accuracy: 0.7563\n",
      "Confusion Matrix:\n",
      "[[ 56  51]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.52      0.66       107\n",
      "         1.0       0.71      0.95      0.81       131\n",
      "\n",
      "    accuracy                           0.76       238\n",
      "   macro avg       0.80      0.73      0.73       238\n",
      "weighted avg       0.79      0.76      0.74       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Avg. Loss: 0.5257\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982]\n",
      "Test Accuracy: 0.7605\n",
      "Confusion Matrix:\n",
      "[[ 57  50]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.53      0.67       107\n",
      "         1.0       0.71      0.95      0.81       131\n",
      "\n",
      "    accuracy                           0.76       238\n",
      "   macro avg       0.80      0.74      0.74       238\n",
      "weighted avg       0.79      0.76      0.75       238\n",
      "\n",
      "Test Accuracy: 0.7605\n",
      "Confusion Matrix:\n",
      "[[ 57  50]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.53      0.67       107\n",
      "         1.0       0.71      0.95      0.81       131\n",
      "\n",
      "    accuracy                           0.76       238\n",
      "   macro avg       0.80      0.74      0.74       238\n",
      "weighted avg       0.79      0.76      0.75       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033, 0.7605042016806722]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Avg. Loss: 0.5049\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604]\n",
      "Test Accuracy: 0.7815\n",
      "Confusion Matrix:\n",
      "[[ 62  45]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.58      0.70       107\n",
      "         1.0       0.73      0.95      0.83       131\n",
      "\n",
      "    accuracy                           0.78       238\n",
      "   macro avg       0.82      0.76      0.77       238\n",
      "weighted avg       0.81      0.78      0.77       238\n",
      "\n",
      "Test Accuracy: 0.7815\n",
      "Confusion Matrix:\n",
      "[[ 62  45]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.58      0.70       107\n",
      "         1.0       0.73      0.95      0.83       131\n",
      "\n",
      "    accuracy                           0.78       238\n",
      "   macro avg       0.82      0.76      0.77       238\n",
      "weighted avg       0.81      0.78      0.77       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033, 0.7605042016806722, 0.7815126050420168]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Avg. Loss: 0.4803\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732]\n",
      "Test Accuracy: 0.7857\n",
      "Confusion Matrix:\n",
      "[[ 65  42]\n",
      " [  9 122]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.61      0.72       107\n",
      "         1.0       0.74      0.93      0.83       131\n",
      "\n",
      "    accuracy                           0.79       238\n",
      "   macro avg       0.81      0.77      0.77       238\n",
      "weighted avg       0.80      0.79      0.78       238\n",
      "\n",
      "Test Accuracy: 0.7857\n",
      "Confusion Matrix:\n",
      "[[ 65  42]\n",
      " [  9 122]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.61      0.72       107\n",
      "         1.0       0.74      0.93      0.83       131\n",
      "\n",
      "    accuracy                           0.79       238\n",
      "   macro avg       0.81      0.77      0.77       238\n",
      "weighted avg       0.80      0.79      0.78       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033, 0.7605042016806722, 0.7815126050420168, 0.7857142857142857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Avg. Loss: 0.4529\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335]\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 73  34]\n",
      " [ 10 121]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.68      0.77       107\n",
      "         1.0       0.78      0.92      0.85       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.83      0.80      0.81       238\n",
      "weighted avg       0.83      0.82      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 73  34]\n",
      " [ 10 121]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.68      0.77       107\n",
      "         1.0       0.78      0.92      0.85       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.83      0.80      0.81       238\n",
      "weighted avg       0.83      0.82      0.81       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033, 0.7605042016806722, 0.7815126050420168, 0.7857142857142857, 0.8151260504201681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Avg. Loss: 0.4460\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 79  28]\n",
      " [ 13 118]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.74      0.79       107\n",
      "         1.0       0.81      0.90      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 79  28]\n",
      " [ 13 118]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.74      0.79       107\n",
      "         1.0       0.81      0.90      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033, 0.7605042016806722, 0.7815126050420168, 0.7857142857142857, 0.8151260504201681, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Avg. Loss: 0.4561\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811]\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 14 117]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.76      0.80       107\n",
      "         1.0       0.82      0.89      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.84      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8319\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 14 117]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.76      0.80       107\n",
      "         1.0       0.82      0.89      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.84      0.83      0.83       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033, 0.7605042016806722, 0.7815126050420168, 0.7857142857142857, 0.8151260504201681, 0.8277310924369747, 0.8319327731092437]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Avg. Loss: 0.4598\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.76      0.79       107\n",
      "         1.0       0.82      0.88      0.85       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.76      0.79       107\n",
      "         1.0       0.82      0.88      0.85       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033, 0.7605042016806722, 0.7815126050420168, 0.7857142857142857, 0.8151260504201681, 0.8277310924369747, 0.8319327731092437, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Avg. Loss: 0.5314\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057, 1.7375279302372757]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.76      0.79       107\n",
      "         1.0       0.82      0.88      0.85       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.76      0.79       107\n",
      "         1.0       0.82      0.88      0.85       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033, 0.7605042016806722, 0.7815126050420168, 0.7857142857142857, 0.8151260504201681, 0.8277310924369747, 0.8319327731092437, 0.8235294117647058, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Avg. Loss: 0.4853\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057, 1.7375279302372757, 1.7828657832006696]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 80  27]\n",
      " [ 14 117]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.75      0.80       107\n",
      "         1.0       0.81      0.89      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 80  27]\n",
      " [ 14 117]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.75      0.80       107\n",
      "         1.0       0.81      0.89      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033, 0.7605042016806722, 0.7815126050420168, 0.7857142857142857, 0.8151260504201681, 0.8277310924369747, 0.8319327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Avg. Loss: 0.5232\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057, 1.7375279302372757, 1.7828657832006696, 1.82720672831708]\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.76      0.79       107\n",
      "         1.0       0.82      0.88      0.85       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Test Accuracy: 0.8235\n",
      "Confusion Matrix:\n",
      "[[ 81  26]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.76      0.79       107\n",
      "         1.0       0.82      0.88      0.85       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.82      0.82      0.82       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033, 0.7605042016806722, 0.7815126050420168, 0.7857142857142857, 0.8151260504201681, 0.8277310924369747, 0.8319327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8235294117647058]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Avg. Loss: 0.4952\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057, 1.7375279302372757, 1.7828657832006696, 1.82720672831708, 1.870617151241471]\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.77      0.80       107\n",
      "         1.0       0.82      0.88      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Test Accuracy: 0.8277\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 16 115]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.77      0.80       107\n",
      "         1.0       0.82      0.88      0.85       131\n",
      "\n",
      "    accuracy                           0.83       238\n",
      "   macro avg       0.83      0.82      0.82       238\n",
      "weighted avg       0.83      0.83      0.83       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033, 0.7605042016806722, 0.7815126050420168, 0.7857142857142857, 0.8151260504201681, 0.8277310924369747, 0.8319327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8235294117647058, 0.8277310924369747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Avg. Loss: 0.5800\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057, 1.7375279302372757, 1.7828657832006696, 1.82720672831708, 1.870617151241471, 1.913159049531643]\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.77      0.78       107\n",
      "         1.0       0.82      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8109\n",
      "Confusion Matrix:\n",
      "[[ 82  25]\n",
      " [ 20 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.77      0.78       107\n",
      "         1.0       0.82      0.85      0.83       131\n",
      "\n",
      "    accuracy                           0.81       238\n",
      "   macro avg       0.81      0.81      0.81       238\n",
      "weighted avg       0.81      0.81      0.81       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033, 0.7605042016806722, 0.7815126050420168, 0.7857142857142857, 0.8151260504201681, 0.8277310924369747, 0.8319327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8235294117647058, 0.8277310924369747, 0.8109243697478992]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Avg. Loss: 0.5690\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057, 1.7375279302372757, 1.7828657832006696, 1.82720672831708, 1.870617151241471, 1.913159049531643, 1.954886542779697]\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 80  27]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.75      0.78       107\n",
      "         1.0       0.81      0.87      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.81       238\n",
      "weighted avg       0.82      0.82      0.81       238\n",
      "\n",
      "Test Accuracy: 0.8151\n",
      "Confusion Matrix:\n",
      "[[ 80  27]\n",
      " [ 17 114]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.75      0.78       107\n",
      "         1.0       0.81      0.87      0.84       131\n",
      "\n",
      "    accuracy                           0.82       238\n",
      "   macro avg       0.82      0.81      0.81       238\n",
      "weighted avg       0.82      0.82      0.81       238\n",
      "\n",
      "Accuracy list is  [0.4495798319327731, 0.4495798319327731, 0.4495798319327731, 0.680672268907563, 0.7436974789915967, 0.634453781512605, 0.6638655462184874, 0.7352941176470589, 0.7436974789915967, 0.7647058823529411, 0.7563025210084033, 0.7605042016806722, 0.7815126050420168, 0.7857142857142857, 0.8151260504201681, 0.8277310924369747, 0.8319327731092437, 0.8235294117647058, 0.8235294117647058, 0.8277310924369747, 0.8235294117647058, 0.8277310924369747, 0.8109243697478992, 0.8151260504201681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Avg. Loss: 0.6933\n",
      "epsilon list is  [0.5071015623125099]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Avg. Loss: 0.6928\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Avg. Loss: 0.6918\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Avg. Loss: 0.6915\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Avg. Loss: 0.6920\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Avg. Loss: 0.6908\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Avg. Loss: 0.6891\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Avg. Loss: 0.6896\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Avg. Loss: 0.6886\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Avg. Loss: 0.6884\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Avg. Loss: 0.6878\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Avg. Loss: 0.6878\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Avg. Loss: 0.6869\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Avg. Loss: 0.6859\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Avg. Loss: 0.6854\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335]\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Test Accuracy: 0.5504\n",
      "Confusion Matrix:\n",
      "[[  0 107]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       107\n",
      "         1.0       0.55      1.00      0.71       131\n",
      "\n",
      "    accuracy                           0.55       238\n",
      "   macro avg       0.28      0.50      0.36       238\n",
      "weighted avg       0.30      0.55      0.39       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Avg. Loss: 0.6825\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297]\n",
      "Test Accuracy: 0.5630\n",
      "Confusion Matrix:\n",
      "[[  3 104]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.03      0.05       107\n",
      "         1.0       0.56      1.00      0.72       131\n",
      "\n",
      "    accuracy                           0.56       238\n",
      "   macro avg       0.78      0.51      0.39       238\n",
      "weighted avg       0.76      0.56      0.42       238\n",
      "\n",
      "Test Accuracy: 0.5630\n",
      "Confusion Matrix:\n",
      "[[  3 104]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.03      0.05       107\n",
      "         1.0       0.56      1.00      0.72       131\n",
      "\n",
      "    accuracy                           0.56       238\n",
      "   macro avg       0.78      0.51      0.39       238\n",
      "weighted avg       0.76      0.56      0.42       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5630252100840336]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Avg. Loss: 0.6812\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811]\n",
      "Test Accuracy: 0.5756\n",
      "Confusion Matrix:\n",
      "[[  6 101]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.06      0.11       107\n",
      "         1.0       0.56      1.00      0.72       131\n",
      "\n",
      "    accuracy                           0.58       238\n",
      "   macro avg       0.78      0.53      0.41       238\n",
      "weighted avg       0.76      0.58      0.45       238\n",
      "\n",
      "Test Accuracy: 0.5756\n",
      "Confusion Matrix:\n",
      "[[  6 101]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.06      0.11       107\n",
      "         1.0       0.56      1.00      0.72       131\n",
      "\n",
      "    accuracy                           0.58       238\n",
      "   macro avg       0.78      0.53      0.41       238\n",
      "weighted avg       0.76      0.58      0.45       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5630252100840336, 0.5756302521008403]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Avg. Loss: 0.6806\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057]\n",
      "Test Accuracy: 0.5966\n",
      "Confusion Matrix:\n",
      "[[ 11  96]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.10      0.19       107\n",
      "         1.0       0.58      1.00      0.73       131\n",
      "\n",
      "    accuracy                           0.60       238\n",
      "   macro avg       0.79      0.55      0.46       238\n",
      "weighted avg       0.77      0.60      0.49       238\n",
      "\n",
      "Test Accuracy: 0.5966\n",
      "Confusion Matrix:\n",
      "[[ 11  96]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.10      0.19       107\n",
      "         1.0       0.58      1.00      0.73       131\n",
      "\n",
      "    accuracy                           0.60       238\n",
      "   macro avg       0.79      0.55      0.46       238\n",
      "weighted avg       0.77      0.60      0.49       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5630252100840336, 0.5756302521008403, 0.5966386554621849]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Avg. Loss: 0.6795\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057, 1.7375279302372757]\n",
      "Test Accuracy: 0.6261\n",
      "Confusion Matrix:\n",
      "[[ 18  89]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.17      0.29       107\n",
      "         1.0       0.60      1.00      0.75       131\n",
      "\n",
      "    accuracy                           0.63       238\n",
      "   macro avg       0.80      0.58      0.52       238\n",
      "weighted avg       0.78      0.63      0.54       238\n",
      "\n",
      "Test Accuracy: 0.6261\n",
      "Confusion Matrix:\n",
      "[[ 18  89]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.17      0.29       107\n",
      "         1.0       0.60      1.00      0.75       131\n",
      "\n",
      "    accuracy                           0.63       238\n",
      "   macro avg       0.80      0.58      0.52       238\n",
      "weighted avg       0.78      0.63      0.54       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5630252100840336, 0.5756302521008403, 0.5966386554621849, 0.6260504201680672]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Avg. Loss: 0.6777\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057, 1.7375279302372757, 1.7828657832006696]\n",
      "Test Accuracy: 0.6218\n",
      "Confusion Matrix:\n",
      "[[ 17  90]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.16      0.27       107\n",
      "         1.0       0.59      1.00      0.74       131\n",
      "\n",
      "    accuracy                           0.62       238\n",
      "   macro avg       0.80      0.58      0.51       238\n",
      "weighted avg       0.78      0.62      0.53       238\n",
      "\n",
      "Test Accuracy: 0.6218\n",
      "Confusion Matrix:\n",
      "[[ 17  90]\n",
      " [  0 131]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.16      0.27       107\n",
      "         1.0       0.59      1.00      0.74       131\n",
      "\n",
      "    accuracy                           0.62       238\n",
      "   macro avg       0.80      0.58      0.51       238\n",
      "weighted avg       0.78      0.62      0.53       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5630252100840336, 0.5756302521008403, 0.5966386554621849, 0.6260504201680672, 0.6218487394957983]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Avg. Loss: 0.6738\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057, 1.7375279302372757, 1.7828657832006696, 1.82720672831708]\n",
      "Test Accuracy: 0.6681\n",
      "Confusion Matrix:\n",
      "[[ 29  78]\n",
      " [  1 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.27      0.42       107\n",
      "         1.0       0.62      0.99      0.77       131\n",
      "\n",
      "    accuracy                           0.67       238\n",
      "   macro avg       0.80      0.63      0.60       238\n",
      "weighted avg       0.78      0.67      0.61       238\n",
      "\n",
      "Test Accuracy: 0.6681\n",
      "Confusion Matrix:\n",
      "[[ 29  78]\n",
      " [  1 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.27      0.42       107\n",
      "         1.0       0.62      0.99      0.77       131\n",
      "\n",
      "    accuracy                           0.67       238\n",
      "   macro avg       0.80      0.63      0.60       238\n",
      "weighted avg       0.78      0.67      0.61       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5630252100840336, 0.5756302521008403, 0.5966386554621849, 0.6260504201680672, 0.6218487394957983, 0.6680672268907563]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Avg. Loss: 0.6709\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057, 1.7375279302372757, 1.7828657832006696, 1.82720672831708, 1.870617151241471]\n",
      "Test Accuracy: 0.7059\n",
      "Confusion Matrix:\n",
      "[[ 38  69]\n",
      " [  1 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.36      0.52       107\n",
      "         1.0       0.65      0.99      0.79       131\n",
      "\n",
      "    accuracy                           0.71       238\n",
      "   macro avg       0.81      0.67      0.65       238\n",
      "weighted avg       0.80      0.71      0.67       238\n",
      "\n",
      "Test Accuracy: 0.7059\n",
      "Confusion Matrix:\n",
      "[[ 38  69]\n",
      " [  1 130]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.36      0.52       107\n",
      "         1.0       0.65      0.99      0.79       131\n",
      "\n",
      "    accuracy                           0.71       238\n",
      "   macro avg       0.81      0.67      0.65       238\n",
      "weighted avg       0.80      0.71      0.67       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5630252100840336, 0.5756302521008403, 0.5966386554621849, 0.6260504201680672, 0.6218487394957983, 0.6680672268907563, 0.7058823529411765]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n",
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Avg. Loss: 0.6689\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057, 1.7375279302372757, 1.7828657832006696, 1.82720672831708, 1.870617151241471, 1.913159049531643]\n",
      "Test Accuracy: 0.7311\n",
      "Confusion Matrix:\n",
      "[[ 49  58]\n",
      " [  6 125]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.46      0.60       107\n",
      "         1.0       0.68      0.95      0.80       131\n",
      "\n",
      "    accuracy                           0.73       238\n",
      "   macro avg       0.79      0.71      0.70       238\n",
      "weighted avg       0.78      0.73      0.71       238\n",
      "\n",
      "Test Accuracy: 0.7311\n",
      "Confusion Matrix:\n",
      "[[ 49  58]\n",
      " [  6 125]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.46      0.60       107\n",
      "         1.0       0.68      0.95      0.80       131\n",
      "\n",
      "    accuracy                           0.73       238\n",
      "   macro avg       0.79      0.71      0.70       238\n",
      "weighted avg       0.78      0.73      0.71       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5630252100840336, 0.5756302521008403, 0.5966386554621849, 0.6260504201680672, 0.6218487394957983, 0.6680672268907563, 0.7058823529411765, 0.7310924369747899]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\anaconda3\\envs\\gpu2\\lib\\site-packages\\opacus\\accountants\\analysis\\prv\\prvs.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  z = np.log((np.exp(t) + q - 1) / q)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Avg. Loss: 0.6630\n",
      "epsilon list is  [0.5071015623125099, 0.6322523402883871, 0.7383869897820592, 0.8325498196810674, 0.917998088708845, 0.996761347117364, 1.0702066368390342, 1.1393015236365494, 1.2047564191761866, 1.267110668543616, 1.3267841623864294, 1.3841107280585982, 1.4393607287173604, 1.4927566914894732, 1.54448405292335, 1.5947001145944297, 1.6435387879239811, 1.6911139088296057, 1.7375279302372757, 1.7828657832006696, 1.82720672831708, 1.870617151241471, 1.913159049531643, 1.954886542779697]\n",
      "Test Accuracy: 0.7395\n",
      "Confusion Matrix:\n",
      "[[ 52  55]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.49      0.63       107\n",
      "         1.0       0.69      0.95      0.80       131\n",
      "\n",
      "    accuracy                           0.74       238\n",
      "   macro avg       0.79      0.72      0.71       238\n",
      "weighted avg       0.78      0.74      0.72       238\n",
      "\n",
      "Test Accuracy: 0.7395\n",
      "Confusion Matrix:\n",
      "[[ 52  55]\n",
      " [  7 124]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.49      0.63       107\n",
      "         1.0       0.69      0.95      0.80       131\n",
      "\n",
      "    accuracy                           0.74       238\n",
      "   macro avg       0.79      0.72      0.71       238\n",
      "weighted avg       0.78      0.74      0.72       238\n",
      "\n",
      "Accuracy list is  [0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5504201680672269, 0.5630252100840336, 0.5756302521008403, 0.5966386554621849, 0.6260504201680672, 0.6218487394957983, 0.6680672268907563, 0.7058823529411765, 0.7310924369747899, 0.7394957983193278]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from opacus import PrivacyEngine\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class HeartDiseaseModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HeartDiseaseModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "input_size = 22  # number of features\n",
    "\n",
    "\n",
    "\n",
    "cleveland = pd.read_csv('C:/Users/siddh/heart_statlog_cleveland_hungary_final.csv')\n",
    "print('Shape of DataFrame: {}'.format(cleveland.shape))\n",
    "print(cleveland.loc[1])\n",
    "\n",
    "cleveland.head()\n",
    "\n",
    "data = cleveland[~cleveland.isin(['?'])]\n",
    "data.loc[280:]\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "# renaming columns\n",
    "data = data.rename(columns={'chest pain type': 'cps', 'resting bp s': 'rbps', 'fasting blood sugar': 'fbs',\n",
    "                            'resting ecg': 'recg', 'max heart rate': 'max_heart_rate', 'exercise angina': 'ex_angina',\n",
    "                            'ST slope': 'STslope'})\n",
    "\n",
    "# dealing with categorical variables for better inference with the model\n",
    "def convert_encoding(data):\n",
    "    dummies = pd.get_dummies(data['sex'], prefix='sex')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('sex', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['STslope'], prefix='STslope')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('STslope', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['ex_angina'], prefix='ex_angina')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('ex_angina', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['recg'], prefix='recg')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('recg', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['cps'], prefix='cps')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('cps', axis=1, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(data['fbs'], prefix='fbs')\n",
    "    data = pd.concat([data, dummies], axis=1)\n",
    "    data.drop('fbs', axis=1, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = convert_encoding(data)\n",
    "\n",
    "y = data['target']\n",
    "X = data.drop(['target'], axis=1)\n",
    "y = y.to_numpy()\n",
    "X = X.to_numpy()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    return torch.from_numpy(df).float()\n",
    "\n",
    "X_traint = df_to_tensor(X_train)\n",
    "y_traint = df_to_tensor(y_train)\n",
    "X_testt = df_to_tensor(X_test)\n",
    "y_testt = df_to_tensor(y_test)\n",
    "\n",
    "train_ds = TensorDataset(X_traint, y_traint)\n",
    "test_ds = TensorDataset(X_testt, y_testt)\n",
    "\n",
    "# create data loaders\n",
    "batch_size = 5\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "delta=1e-7\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, optimizer, epoch, epsilon_values):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        target = target.unsqueeze(1).float()\n",
    "        target = target.repeat(1, output.shape[1])\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    epsilon = float(privacy_engine.get_epsilon(delta))\n",
    "    epsilon_values.append(epsilon)\n",
    "    print('Epoch: {}, Avg. Loss: {:.4f}'.format(epoch, np.mean(losses)))\n",
    "    return epsilon_values\n",
    "\n",
    "def test(model, test_dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            output = model(data)\n",
    "            predicted = torch.round(output)\n",
    "            predictions.extend(predicted.tolist())\n",
    "            targets.extend(target.tolist())\n",
    "    acc = accuracy_score(targets, predictions)\n",
    "    print('Test Accuracy: {:.4f}'.format(acc))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(targets, predictions))\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(targets, predictions))\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "x=[\"adam\",\"SGD\"]\n",
    "for i,j in enumerate(x):\n",
    "    accuracies = []\n",
    "    epsilon_list=[]\n",
    "    epsilon_values = []\n",
    "    epsilon_list_total=[]\n",
    "    model1 = HeartDiseaseModel(input_size)\n",
    "    model2 = HeartDiseaseModel(input_size)\n",
    "    if(i==0):\n",
    "        model=model1\n",
    "    elif (i==1):\n",
    "        model=model2\n",
    "    privacy_engine = PrivacyEngine()\n",
    "    loss_fn = nn.BCELoss() # Binary Cross Entropy\n",
    "    if(j==\"adam\"):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    model, optimizer, dataloader = privacy_engine.make_private_with_epsilon(module=model,optimizer=optimizer,data_loader=train_dataloader,\n",
    "                                                         max_grad_norm=1.0,target_epsilon=2, epochs=25, target_delta= 1e-7)\n",
    "    criterion = nn.BCELoss()\n",
    "    for epoch in range(1,25):\n",
    "        epsilon_list=train(model, train_dataloader, optimizer, epoch,epsilon_values)\n",
    "        print('epsilon list is ', epsilon_list)\n",
    "        test(model, test_dataloader)\n",
    "        acc = test(model, test_dataloader)\n",
    "        accuracies.append(acc)\n",
    "        print('Accuracy list is ',accuracies)\n",
    "    \n",
    "    if (i==0):\n",
    "        accuracies=[i*100 for i in accuracies]\n",
    "        plt.plot(epsilon_list, accuracies,color=\"red\",label='ADAM optimizer')\n",
    "    elif(i==1):\n",
    "        accuracies=[i*100 for i in accuracies]\n",
    "        plt.plot(epsilon_list, accuracies,color=\"green\", label='SGD optimizer',linestyle='dashed')  \n",
    "    del model\n",
    "\n",
    "plt.xticks(range(0,10)) \n",
    "plt.yticks(range(int(accuracies[0]),100,3)) \n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Change in Accuracy with Epsilon')\n",
    "plt.legend(loc ='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390617e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
